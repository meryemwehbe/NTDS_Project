{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimenal Analysis On Facebook Posts\n",
    "\n",
    "This project will use sentimental analysis to study the reaction of people on particular Facebook controvesial posts.\n",
    "To do this, Several classifiers were trained on datasets and the one with best performance were chosen to do the study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Acquisition\n",
    "We used two main datasets for training:\n",
    "1. ** Sentimental Analysis Dataset:** 1.6 million collected labelled sentences with '0' for negative and '1' for positive. This dataset is used to train a 2 class classifier.\n",
    "\n",
    " **Sample:**\n",
    "\n",
    "            [['0' 'I missed the New Moon trailer...']\n",
    "            ['1' 'omg its already 7:30 :O']\n",
    "            ['0' \".. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just \n",
    "            get a crown put on (30mins)...\"]\n",
    "            ['0' '         i think mi bf is cheating on me!!!       T_T']\n",
    "            ['0' '         or i just worry too much?        ']]\n",
    " \n",
    "1. ** Amazon Dataset:** 30k Amazon Product reviews from users with the rating given to the product. This dataset is used to train a multi-class classifier and Regressors.\n",
    "\n",
    "  **Sample:**\n",
    "  \n",
    "              [[2, \"It is good if you have internet than you can download the stuff, else, you can't  \"],\n",
    "      \n",
    "              [5, \"The RIO rocks! It is so great that Diamond Multimedia prevailed in their fight against the forces of pure evil in the music industry and allowed us, the public, to have the RIO! This little baby holds your MP3's and plays  them with outrageous quality and no moving parts! You simply cannot make  the music &quot;skip&quot;. Take it jogging, bob sledding, whatever! The  Rio is cute and compact, battery lasts forever, runs great and is really  simple to use. Works well with the PC linkup, etc. A hot item!  \"],\n",
    "      \n",
    "              [4, 'I had high hopes for the Diamond Rio and it certainly lived up 2 the hype. Lightweight and excellent quality with some good connecting software. My only gripe can be with memory. You definately need another 32Mb to store  your music. If you want one, my advice is to wait for the new upgraded  version with 64Mb and a graphic equaliser!  '], [5, \"Diamond's RIO is the current, silicon-state nightmare for monopolistic entertainment industries. When the first audio recording-devices entered the consumer market decades ago, the idea of a controlled  &quot;charge-per-copy&quot; business model in the music industry was  doomed. Although traditional copyrights could never be totally enforced,  the record and music-producing industry neglected these threads existing in  the shades of multi-billion profits.<p>Mp3 and the Internet raided the  existing markets with their &quot;natural&quot; power like a cruise-missile  against a frying-pan.  Like other good examples for  &quot;Killer-Apps&quot;, the RIO is designed to Web-Specs (mp3). This makes  its use and performance comparable with other CD-quality playing devices  while putting the power of the internet into the palm of your hand. If you  want to make a statement of &quot;being digital&quot;, the RIO is a must!  \"],\n",
    "      \n",
    "              [5, \"Remember when the Walkman hit the market years ago?  At first it was fad, then it became a craze before finally becoming as ubiquitous as taxi cabs in Manhattan.  The Diamond Rio is the first product with the potential to  eventually REPLACE the Walkman.  Think about it - portable music that  sounds as clear as a compact disc from a device that looks and feels like a  pager.  Unshockable, no matter how much you jump around or how bumpy the  bus ride.  This is the perfect gift for your favorite &quot;gadget  junkie&quot;, as long as they have a PC to download from.  Pssst...here's a  secret:  you can record your own CD's into MP3 format and download the  songs into your RIO - it isn't just for internet music!!  Great product.  \"]]\n",
    "              \n",
    "1. ** Facebook Dataset:** In order to study the reactions of the Facebook audience to a post , we needed to scrap to comments of this post.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postReactions (postId,postType,filename):\n",
    "    ...\n",
    "    fields = 'id,created_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'    \n",
    "    url = 'https://graph.facebook.com/{}?fields={}&access_token={}'.format(postId, fields, token)\n",
    "    \n",
    "    fb = pd.DataFrame(columns=['post message','post id','post time','post likes','nb of comments','comment id', 'comment message', 'comment time', 'comment likes'])# 'user name']) #'age', 'gender','location','political','religion','education'])\n",
    "    serie={'post message':[],'post id':[],'post time':[],'post likes':[],'nb of comments':[],'comment id':[],'comment message':[],'comment time':[], 'comment likes':[]}#,'user name':[]} # 'age':[], 'gender':[],'location':[],'political':[],'religion':[],'education':[]};\n",
    "  \n",
    "    post = requests.get(url).json()\n",
    "    ...      \n",
    "        try:\n",
    "            # Only work with posts with comments which have text.\n",
    "            nb_comments_per_post=post['comments']['summary']['total_count']\n",
    "             \n",
    "            #IndexError if no comment on the page, only work with posts\n",
    "            # which have at least 1 comment\n",
    "            x= post['comments']['data'][0]['message']\n",
    "        \n",
    "            serie['post message']=post_message\n",
    "            serie['post time']=post['created_time'] \n",
    "            serie['post likes']=post['likes']['summary']['total_count']\n",
    "            serie['nb of comments']= post['comments']['summary']['total_count']\n",
    "            serie['post id']=post['id']\n",
    "        \n",
    "            fb = getAllComments(postId,nb_comments_per_post,serie,fb)\n",
    "                                     \n",
    "        except IndexError or KeyError:\n",
    "            print('')\n",
    "    except KeyError:\n",
    "        print('')\n",
    "    ....\n",
    "        \n",
    "    fb['post time'] = fb['post time'].apply(convert_time)\n",
    "    fb['comment time'] = fb['comment time'].apply(convert_time)\n",
    "    \n",
    "    ...\n",
    "    return fb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lastPostsReactions(page,filename,n):\n",
    "    ...\n",
    "    i=0\n",
    "    while i < n: #len(fb) < n:\n",
    "    \n",
    "        posts = requests.get(url).json()\n",
    "        \n",
    "        # extract information for each of the received post\n",
    "        for post in posts['data']:\n",
    "           \n",
    "           [...]\n",
    "        try:\n",
    "            url = posts['paging']['next']\n",
    "            #print('next')\n",
    "        except KeyError:\n",
    "            #print('no next')\n",
    "            break  \n",
    "\n",
    "    print(\"Number of posts: \",i)\n",
    "                            \n",
    "    return fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction\n",
    "\n",
    "For the classification , the comments are transformed into vectors that represent the features of the text. This is also called the **bag-of-words model**. Each word is represented by an integer and count the appearance in the document.\n",
    "\n",
    "+ **Removing Stopwords:** Using the English dictionary\n",
    "\n",
    "+ **Select K Best Model:** Selects the K best features in relation with the labels using the chi2 distribution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_bag_of_words(text, stopwords , vocab=None):\n",
    "    vectorizer = CountVectorizer(stop_words = stopwords,vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(text)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    return vectors, vocabulary\n",
    "\n",
    "KBestModel = SelectKBest(chi2, k=1000).fit(bow, Y_sat)  \n",
    "indices = KBestModel.get_support(True)\n",
    "bow_transformed = KBestModel.transform(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Some of the informative features of our dataset:\n",
    "\n",
    "+  'adorable\n",
    "+ 'afraid'\n",
    "+ 'agree'\n",
    "+ 'attacked'\n",
    "+ 'awesome'\n",
    "+ 'awesomeness'\n",
    "+ 'awful'\n",
    "+ 'aww'\n",
    "+ 'badly'\n",
    "+ 'beautiful'\n",
    "+ 'celebrate'\n",
    "+ 'depressed'\n",
    "+ 'failed'\n",
    "+ 'fucked' \n",
    "+ 'happiness'\n",
    "+ 'jealous'\n",
    "+  'peace'\n",
    "+ 'success'\n",
    "+ 'suck'\n",
    "+ 'waste'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Transform_To_Input_Format_SAT_Classifiers(X):\n",
    "    with codecs.open('Best_Features_SAT.txt','r',encoding='utf8') as f:\n",
    "        features = f.readlines()\n",
    "    features = [x.strip(\"\\n\") for x in features]\n",
    "    X_transformed,vocab = compute_bag_of_words(X, stopwords.words(),features)\n",
    "    return X_transformed,features\n",
    "\n",
    "def Transform_To_Input_Format_Amazon(X):\n",
    "    with open('Best_Features_Amazon.txt') as f:\n",
    "        features = f.readlines()\n",
    "    features = [x.strip(\"\\n\") for x in features]\n",
    "    X_transformed,vocab = compute_bag_of_words(X, stopwords.words(),features)\n",
    "    return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partial_fit_classifiers = {\n",
    "    'SGD': SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "        eta0=0.0, fit_intercept=True, l1_ratio=0,\n",
    "        learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
    "        penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
    "        verbose=0, warm_start=False),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'NB Multinomial': MultinomialNB(alpha=0.01),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(C=1.0, n_iter=50, shuffle=True, \n",
    "                                                      verbose=0, loss='hinge',\n",
    "                                                      warm_start=False),\n",
    "    'NB Bernoulli': BernoulliNB(alpha=0.01),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partial_fit_Regressors = {\n",
    "    'SGD Regressor':SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.001, l1_ratio=0, \n",
    "                                 fit_intercept=True, n_iter=1000, shuffle=True, verbose=0, epsilon=0.01, random_state=None,\n",
    "                                 learning_rate='invscaling', eta0=0.01, power_t=0.25, warm_start=False, average=False),\n",
    "    'Passive-Aggressive Regressor' : PassiveAggressiveRegressor(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental learning\n",
    "\n",
    "The dataset is too large to train the classifiers at once **Memory Error** that is why we used incremental learning:\n",
    "\n",
    "** Libraries used: **\n",
    "    1. sklearn\n",
    "    2. Numpy\n",
    "    3. Scipy\n",
    "\n",
    "\n",
    "** Learning Process: **\n",
    "\n",
    "1. Divide Data into Batches\n",
    "        minibatch_size = 10000\n",
    "        batch = bow_transformed[start:start+minibatch_size]\n",
    "\n",
    "2. Partially train the classifier on the batch\n",
    "        \n",
    "        cls.partial_fit(X_train, Y_train, classes = y_all)\n",
    "\n",
    "3. Cross-Validation\n",
    "        kf = KFold(n_splits = 10)\n",
    "        for train_index,test_index in kf.split(X):\n",
    "            X_train,X_test = X[train_index],X[test_index]\n",
    "            Y_train,Y_test = Y[train_index],Y[test_index]\n",
    "4. Accuracy Check:\n",
    "\n",
    "   Classifiers: \n",
    "        100*sklearn.metrics.accuracy_score(Y_test, train_pred)\n",
    "   Regressors:\n",
    "        sklearn.metrics.mean_squared_error(Y_test, train_pred)\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on SAT dataset\n",
    "\n",
    "\n",
    "<img src= \"DifferentAccuraciesSAT.png\" style=\"margin: 0px 0px 5px 20px; width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"OverallAccuraciesSAT.png\" style=\"margin: 0px 0px 5px 20px; width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on Amazon Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"AccuraciesAmazon.png\" style=\"margin: 0px 0px 5px 20px; width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"OverallAccuraciesAmazon.png\" style=\"margin: 0px 0px 5px 20px; width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"OverallRegressionResults.png\" style=\"margin: 0px 0px 5px 20px; width: 800px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
