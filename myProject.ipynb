{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.io\n",
    "import csv\n",
    "from IPython.display import display\n",
    "from scipy import sparse\n",
    "import os.path\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier,SGDRegressor,PassiveAggressiveRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"stopwords\") \n",
    "nltk.download('punkt')\n",
    "#removing stopwords\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "## drawing tools\n",
    "# Load libraries\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "\n",
    "# Visualization \n",
    "%matplotlib notebook \n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy import ndimage\n",
    "\n",
    "# High-res visualization (but no rotation possible)\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "\n",
    "# Print output of LFR code\n",
    "import subprocess\n",
    "\n",
    "# Sparse matrix\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "# 3D visualization\n",
    "import pylab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Import data\n",
    "import scipy.io\n",
    "\n",
    "# Import functions in lib folder\n",
    "import sys\n",
    "sys.path.insert(1, 'lib')\n",
    "\n",
    "# Import helper functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import distance function\n",
    "import sklearn.metrics.pairwise\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_between_r( s, first, last ):\n",
    "    try:\n",
    "        start = s.rindex( first ) + len( first )\n",
    "        end = s.rindex( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sentimental Analysis Text dataset: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SAT_data =[]\n",
    "import codecs\n",
    "#with open('Projectdataset/Sentiment Analysis Dataset.csv', 'r') as csvfile:\n",
    "with codecs.open('Projectdataset/Sentiment Analysis Dataset.csv','r',encoding='utf8') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')#, quotechar='|')\n",
    "    for row in spamreader:\n",
    "        SAT_data.append(row) \n",
    "\n",
    "SAT_data = SAT_data[2:]\n",
    "SAT_data = [[row[1],row[3]] for row in SAT_data]\n",
    "SAT_data = np.array(SAT_data)\n",
    "#SAT_data[:,0] = [int(float(x))*5 for x in SAT_data[:,0]]\n",
    "print('Length = {} '.format(len(SAT_data)))\n",
    "print('Type = {}'.format(type(SAT_data)))\n",
    "print(SAT_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Amazon dataset: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Amazon_data = []\n",
    "file4 = open(\"Projectdataset/amazonMP3reviews/amazon_mp3\", \"r\")\n",
    "data4 = file4.read()\n",
    "\n",
    "data4 = data4.split(\"#####\")\n",
    "for i in range(1,len(data4)):\n",
    "    text = find_between_r( data4[i], \"[fullText]:\", \"[rating]\" )\n",
    "    text = text.replace(\"\\n\",'')\n",
    "    rating = find_between_r( data4[i],\"[rating]:\",\"[recommend]\")\n",
    "    Amazon_data.append([float(rating),text])\n",
    "Amazon_data = np.asarray(Amazon_data)\n",
    "Amazon_data_chunk = Amazon_data[0:8000]\n",
    "Amazon_data[:,0] = [int(float(x)) for x in Amazon_data[:,0]]\n",
    "print('Length = {} '.format(len(Amazon_data)))\n",
    "print('Type = {}'.format(type(Amazon_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Classifier trained on SAT dataset\n",
    "** Feature Extraction\n",
    "** \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "def compute_bag_of_words(text, stopwords , vocab=None):\n",
    "    vectorizer = CountVectorizer(stop_words = stopwords,vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(text)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    return vectors, vocabulary\n",
    "\n",
    "\n",
    "\n",
    "text_sat = column(SAT_data,1)\n",
    "Y_sat = np.asarray(column(SAT_data,0))\n",
    "  \n",
    "bow, vocab = compute_bag_of_words(text_sat, stopwords.words())\n",
    "#KBestModel = SelectKBest(chi2, k=1000).fit(bow, Y_sat)  \n",
    "#indices = KBestModel.get_support(True)\n",
    "#bow_transformed = KBestModel.transform(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "KBestModel = SelectKBest(chi2, k=1000).fit(bow, Y_sat)  \n",
    "indices = KBestModel.get_support(True)\n",
    "bow_transformed = KBestModel.transform(bow)\n",
    "#sving best features\n",
    "print(\"bow = {}\".format(bow.shape))\n",
    "print(\"bow_transformed = {}\".format(bow_transformed.shape))\n",
    "best_features_Sat = np.array(vocab)[indices]\n",
    "print(len(best_features_Sat))\n",
    "\n",
    "file = codecs.open('Best_Features_SAT.txt', 'w',encoding='utf8')\n",
    "for word in best_features_Sat:\n",
    "    file.write(\"%s\\n\" % word)    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input X is list of strings \n",
    "def Transform_To_Input_Format_SAT_Classifiers(X):\n",
    "    with codecs.open('Best_Features_SAT.txt','r',encoding='utf8') as f:\n",
    "        features = f.readlines()\n",
    "    features = [x.strip(\"\\n\") for x in features]\n",
    "    X_transformed,vocab = compute_bag_of_words(X, stopwords.words(),features)\n",
    "    return X_transformed,features\n",
    "#Example how to use\n",
    "Xinput,f = Transform_To_Input_Format_SAT_Classifiers([\"i am feeling terrible today\",\"I don't like this\"])\n",
    "print(Xinput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_stats = {'n_test': 0, 'n_test_pos': 0}\n",
    "def progress(cls_name, stats):\n",
    "    \"\"\"Report progress information, return a string.\"\"\"\n",
    "    duration = time.time() - stats['t0']\n",
    "    s = \"%20s classifier : \\t\" % cls_name\n",
    "    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\n",
    "    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\n",
    "    s += \"accuracy: %(accuracy).3f \" % stats\n",
    "    s += \"in %.2fs (%5d docs/s)\" % (duration, stats['n_train'] / duration)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "batches = []\n",
    "y_all = np.unique(Y_sat)\n",
    "\n",
    "\n",
    "minibatch_size = 10000\n",
    "# divide data into batches\n",
    "i = 0\n",
    "start = 0\n",
    "while(start < bow_transformed.shape[0]):\n",
    "    if(start + minibatch_size < bow_transformed.shape[0]):\n",
    "        batch = bow_transformed[start:start+minibatch_size]\n",
    "        batches.append(batch)\n",
    "        start+=minibatch_size\n",
    "    else:\n",
    "        batch = bow_transformed[start:]\n",
    "        batches.append(batch)\n",
    "        start+=minibatch_size\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "#Classifiers      \n",
    "partial_fit_classifiers = {\n",
    "    'SGD': SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "        eta0=0.0, fit_intercept=True, l1_ratio=0,\n",
    "        learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
    "        penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
    "        verbose=0, warm_start=False),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'NB Multinomial': MultinomialNB(alpha=0.01),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(C=1.0, n_iter=50, shuffle=True, \n",
    "                                                      verbose=0, loss='hinge',\n",
    "                                                      warm_start=False),\n",
    "    'NB Bernoulli': BernoulliNB(alpha=0.01),\n",
    "}\n",
    "                   \n",
    "classifier_information = {\n",
    "     'SGD':[],\n",
    "    'Perceptron': [],\n",
    "    'NB Multinomial': [],\n",
    "    'Passive-Aggressive': [],\n",
    "    'NB Bernoulli':[],\n",
    "}       \n",
    "                  \n",
    "\n",
    "\n",
    "number_minibatch = len(batches)\n",
    "total_vect_time = 0.0\n",
    "    \n",
    "print(number_minibatch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FirstBatch = True\n",
    "k = 0;\n",
    "# Main loop : iterate on mini-batches of examples\n",
    "for i in range(number_minibatch):\n",
    "    print(\"Batch number = {}\".format(i))\n",
    "    batch = batches[i]\n",
    "    X = batch\n",
    "    if(k + minibatch_size < len(Y_sat)):\n",
    "        Y = Y_sat[k:k+minibatch_size]\n",
    "    else:\n",
    "        Y = Y_sat[k:]\n",
    "    k = k+minibatch_size\n",
    "    #classifiers\n",
    "    for cls_name, cls in partial_fit_classifiers.items():\n",
    "        #cross_validation\n",
    "        kf = KFold(n_splits = 10)\n",
    "        results =[]\n",
    "        for train_index,test_index in kf.split(X):\n",
    "            X_train,X_test = X[train_index],X[test_index]\n",
    "            Y_train,Y_test = Y[train_index],Y[test_index]\n",
    "            if(FirstBatch):\n",
    "                cls.partial_fit(X_train, Y_train, classes = y_all)\n",
    "            else:\n",
    "                cls.partial_fit(X_train, Y_train)\n",
    "            train_pred = cls.predict(X_test)\n",
    "            results.append(100*sklearn.metrics.accuracy_score(Y_test, train_pred))\n",
    "        classifier_information[cls_name].append(np.mean(np.array(results)))\n",
    "    FirstBatch = False\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(classifier_information['SGD'])\n",
    "#x = column(accuracy_info, 0)\n",
    "#y = column(accuracy_info, 1)\n",
    "#plt.plot(classifier_information['Preception'])\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(partial_fit_classifiers['SGD'], 'SGDClassifierOnSat.pkl')\n",
    "joblib.dump(partial_fit_classifiers['Perceptron'], 'PerceptronClassifierOnSat.pkl')\n",
    "joblib.dump(partial_fit_classifiers['NB Multinomial'], 'NBMClassifierOnSat.pkl')\n",
    "joblib.dump(partial_fit_classifiers['Passive-Aggressive'], 'PAClassifierOnSat.pkl')\n",
    "joblib.dump(partial_fit_classifiers['NB Bernoulli'], 'NBBClassifierOnSat.pkl')\n",
    "\n",
    "x = range(0,158);\n",
    "i = 0\n",
    "fig, axes = plt.subplots(2, 3, squeeze=True, figsize=(15, 8))\n",
    "for item in classifier_information:\n",
    "    z = np.polyfit(x, classifier_information[item], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[i//3,i%3].plot(x,p(x),\"r--\")\n",
    "    axes[i//3,i%3].plot(x,classifier_information[item])\n",
    "    axes[i//3,i%3].set_title(item)\n",
    "    axes[i//3,i%3].set_ylabel(\"accuracy\")\n",
    "    axes[i//3,i%3].set_xlabel(\"training Size\")\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(classifier_information['SGD'],label ='SGD')\n",
    "plt.plot(classifier_information['Perceptron'],label = 'Perception')\n",
    "plt.plot(classifier_information['NB Multinomial'],label='NB Multinomial')\n",
    "plt.plot(classifier_information['Passive-Aggressive'],label='Passive-Aggressive')\n",
    "plt.plot(classifier_information['NB Bernoulli'],label='NB Bernoulli')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#plt.legend(['SGD','Perceptron','NB Multinomial','Passive-Aggressive','NB Bernoulli'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier For Amazon data \n",
    "** Feature extraction **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_bag_of_words(text, stopwords, vocab=None):\n",
    "    vectorizer = CountVectorizer(stop_words = stopwords,vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(text)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    return vectors, vocabulary\n",
    "\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "\n",
    "text_amazon = column(Amazon_data,1)\n",
    "Y_amazon = column(Amazon_data,0)\n",
    "\n",
    "\n",
    "bow, vocab = compute_bag_of_words(text_amazon, stopwords.words())\n",
    "KBestModel = SelectKBest( k=1000).fit(bow, Y_amazon) \n",
    "indices = KBestModel.get_support(True)\n",
    "bow_transformed = KBestModel.transform(bow)\n",
    "print(\"bow = {}\".format(bow.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "best_features_Amazon = np.array(vocab)[indices]\n",
    "print(len(best_features_Amazon))\n",
    "\n",
    "file = open('Best_Features_Amazon.txt', 'w')\n",
    "for word in best_features_Amazon:\n",
    "    file.write(\"%s\\n\" % word)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input X is list of strings\n",
    "def Transform_To_Input_Format_Amazon(X):\n",
    "    with open('Best_Features_Amazon.txt') as f:\n",
    "        features = f.readlines()\n",
    "    features = [x.strip(\"\\n\") for x in features]\n",
    "    X_transformed,vocab = compute_bag_of_words(X, stopwords.words(),features)\n",
    "    return X_transformed\n",
    "    \n",
    "print(Transform_To_Input_Format_Amazon([\"I am having a wonderfull day\",\"Thank you very much\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classifier **\n",
    "\n",
    "We train 6 different classifiers on each of the datasets to compare the performance of each classifier:\n",
    "K-NN\n",
    "SVM\n",
    "Random Forest Classifier\n",
    "Ridge Classifier\n",
    "Bernoulli Naive Bayes\n",
    "Mutlinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batches = []\n",
    "Ys=[]\n",
    "Y_amazon = np.array(Y_amazon)\n",
    "y_all = np.unique(np.asarray(column(Amazon_data,0)))\n",
    "print(y_all)\n",
    "minibatch_size = 10000\n",
    "start=0\n",
    "# divide data into batches\n",
    "while(start < bow_transformed.shape[0]):\n",
    "    if(start + minibatch_size < bow_transformed.shape[0]):\n",
    "        batch = bow_transformed[start:start+minibatch_size]\n",
    "        Ys.append(Y_amazon[start:start+minibatch_size])\n",
    "        batches.append(batch)\n",
    "        start+=minibatch_size\n",
    "    else:\n",
    "        batch = bow_transformed[start:]\n",
    "        Ys.append(Y_amazon[start:])\n",
    "        batches.append(batch)\n",
    "        start+=minibatch_size\n",
    "    i += 1\n",
    "\n",
    "        \n",
    "#Classifiers      \n",
    "partial_fit_Regressors = {\n",
    "    'SGD Regressor':SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.001, l1_ratio=0, \n",
    "                                 fit_intercept=True, n_iter=1000, shuffle=True, verbose=0, epsilon=0.01, random_state=None,\n",
    "                                 learning_rate='invscaling', eta0=0.01, power_t=0.25, warm_start=False, average=False),\n",
    "    'Passive-Aggressive Regressor' : PassiveAggressiveRegressor(),\n",
    "     'SGD': SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "        eta0=0.0, fit_intercept=True, l1_ratio=0,\n",
    "        learning_rate='optimal', loss='hinge', n_iter=300, n_jobs=1,\n",
    "        penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
    "        verbose=0, warm_start=False),\n",
    "    'Perceptron': Perceptron(penalty='l1', alpha=0.0001, fit_intercept=True, n_iter=50, \n",
    "                             shuffle=True, verbose=0, eta0=1.0, n_jobs=1, random_state=0, \n",
    "                             class_weight=None, warm_start=False),\n",
    "    'NB Multinomial': MultinomialNB(alpha=1),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(),\n",
    "    'NB Bernoulli': BernoulliNB(alpha=0.01),\n",
    "    \n",
    "}\n",
    "Regressors_information = {\n",
    "    'SGD Regressor':[],\n",
    "    'Passive-Aggressive Regressor':[],\n",
    "    'SGD':[],\n",
    "    'Perceptron': [],\n",
    "    'NB Multinomial': [],\n",
    "    'Passive-Aggressive': [],\n",
    "    'NB Bernoulli':[],\n",
    "}   \n",
    "                   \n",
    "cls_stats = {}       \n",
    "                  \n",
    "\n",
    "number_minibatch = len(batches)\n",
    "    \n",
    "print(number_minibatch)\n",
    "FirstBatch = True\n",
    "k = 0\n",
    "# Main loop : iterate on mini-batches of examples\n",
    "for i in range(number_minibatch):\n",
    "    print(\"Batch number = {}\".format(i))\n",
    "    batch = batches[i]\n",
    "    X = batch\n",
    "    Y = Ys[i]\n",
    "    #classifiers\n",
    "    for cls_name, cls in partial_fit_Regressors.items():\n",
    "        tick = time.time()\n",
    "        #cross_validation\n",
    "        kf = KFold(n_splits = 10)\n",
    "        results =[]\n",
    "        for train_index,test_index in kf.split(X):\n",
    "            X_train,X_test = X[train_index],X[test_index]\n",
    "            Y_train,Y_test = Y[train_index],Y[test_index]\n",
    "            if(FirstBatch and cls_name != \"SGD Regressor\" and cls_name != \"Passive-Aggressive Regressor\"):\n",
    "                cls.partial_fit(X_train, Y_train, classes = y_all)\n",
    "                train_pred = cls.predict(X_test)\n",
    "            elif(cls_name != \"SGD Regressor\" and cls_name != \"Passive-Aggressive Regressor\"):\n",
    "                    cls.partial_fit(X_train, Y_train)\n",
    "                    train_pred = cls.predict(X_test)\n",
    "                    results.append(100*sklearn.metrics.accuracy_score(Y_test, train_pred))               \n",
    "            else:\n",
    "                Y_train = [int(x) for x in Y_train]\n",
    "                Y_test = [int(x) for x in Y_test]\n",
    "                cls.partial_fit(X_train, Y_train)\n",
    "                train_pred = cls.predict(X_test)\n",
    "                results.append(sklearn.metrics.mean_squared_error(Y_test, train_pred))\n",
    "        Regressors_information[cls_name].append(np.mean(np.array(results)))\n",
    "    FirstBatch = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Accuracy of Classifiers **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(partial_fit_Regressors['SGD'], 'SGDClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['Perceptron'], 'PerceptronClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['NB Multinomial'], 'NBMClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['Passive-Aggressive'], 'PAClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['NB Bernoulli'], 'NBBClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['SGD Regressor'], 'SGDRegressorOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['Passive-Aggressive Regressor'], 'Passive-AggressiveRegressorOnAmazon.pkl')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, squeeze=True, figsize=(15, 8))\n",
    "i = 0\n",
    "for item in Regressors_information:\n",
    "    axes[i//4,i%4].plot(Regressors_information[item])\n",
    "    axes[i//4,i%4].set_title(item)\n",
    "    axes[i//4,i%4].set_ylabel(\"accuracy\")\n",
    "    axes[i//4,i%4].set_xlabel(\"training Size\")\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Regressors_information['SGD'],label='SGD')\n",
    "plt.plot(Regressors_information['Passive-Aggressive'],label='Passive-Aggressive')\n",
    "plt.plot(Regressors_information['NB Bernoulli'],label='NB Bernoulli')\n",
    "plt.plot(Regressors_information['Perceptron'],label='Perceptron')\n",
    "plt.plot(Regressors_information['NB Multinomial'],label='NB Multinomial')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Regressors_information['SGD Regressor'],label='SGD Regressor')\n",
    "plt.plot(Regressors_information['Passive-Aggressive Regressor'],label='Passive-Aggressive Regressor')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets from Facebook and Twitter\n",
    "\n",
    "Goal: collect comments from posts on Facebook and Twitter to evaluate the response of the audience (positive,negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "credentials = configparser.ConfigParser()\n",
    "credentials.read('credentials.ini')\n",
    "token = credentials.get('facebook', 'token')\n",
    "\n",
    "import requests  \n",
    "import facebook \n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "#testing with EPFL page, run it with the page of a politician's page\n",
    "page = 'EPFL.ch'   #nb comments #nb likes #location  #comments likes \n",
    "postId= 10155100406682590\n",
    "\n",
    "#prepare data\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_time(row):\n",
    "    return datetime.strptime(row, '%Y-%m-%dT%H:%M:%S+0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAllComments(postId,nb_comments_per_post,serie,fb):\n",
    "    fields_comments = 'comment_count,like_count,created_time,message'\n",
    "    url_comments = 'https://graph.facebook.com/{}/comments/?fields={}&access_token={}'.format(postId, fields_comments, token)\n",
    "    \n",
    "    comment =0\n",
    "    #print(nb_comments_per_post)\n",
    "    while comment <= nb_comments_per_post + 1:\n",
    "    \n",
    "        post_comments=requests.get(url_comments).json()\n",
    "        \n",
    "        \n",
    "        #print('len ',len(post_comments['data']))\n",
    "        i=0\n",
    "        for com in post_comments['data']:\n",
    "            i=i+1\n",
    "           \n",
    "            comment_message=com['message']\n",
    "            serie['comment message'] = comment_message\n",
    "            serie['comment time'] = com['created_time']\n",
    "            serie['comment likes'] =  com['like_count']\n",
    "            serie['comment id']=com['id']\n",
    "\n",
    "            fb = fb.append(serie, ignore_index=True)\n",
    "            comment=comment+1\n",
    "        #print('i',i)\n",
    "        #print(fb['comment message'])\n",
    "        try:\n",
    "            url_comments = post_comments['paging']['next']\n",
    "        except KeyError:\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return fb\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def lastPostsReactions(page,filename,n):\n",
    "    fields = 'id,created_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'\n",
    "    url = 'https://graph.facebook.com/{}/posts?fields={}&access_token={}'.format(page, fields, token)\n",
    "\n",
    "    fb = pd.DataFrame(columns=['post message','post id','post time','post likes','nb of comments','comment id', 'comment message', 'comment time', 'comment likes'])# 'user name']) #'age', 'gender','location','political','religion','education'])\n",
    "    serie={'post message':[],'post id':[],'post time':[],'post likes':[],'nb of comments':[],'comment id':[],'comment message':[],'comment time':[], 'comment likes':[]}#,'user name':[]} # 'age':[], 'gender':[],'location':[],'political':[],'religion':[],'education':[]};\n",
    "    \n",
    "    i=0\n",
    "    while i < n: #len(fb) < n:\n",
    "    \n",
    "        posts = requests.get(url).json()\n",
    "        \n",
    "        # extract information for each of the received post\n",
    "        for post in posts['data']:\n",
    "           \n",
    "            try:\n",
    "                # Only work with posts with text.\n",
    "                post_message = post['message']\n",
    "                postId=post['id']\n",
    "                try:\n",
    "                    # Only work with posts with comments which have text.\n",
    "                    nb_comments_per_post=post['comments']['summary']['total_count']\n",
    "                \n",
    "                    x= post['comments']['data'][0]['message'] #IndexError if no comment on the page, only work with posts\n",
    "                    # which have at least 1 comment\n",
    "                    i=i+1\n",
    "    \n",
    "                    post_message=post['message']\n",
    "                    serie['post message']=post_message\n",
    "                    serie['post time']=post['created_time']\n",
    "                    serie['post likes']=post['likes']['summary']['total_count']\n",
    "                    serie['nb of comments']= post['comments']['summary']['total_count']\n",
    "                    serie['post id']=post['id']\n",
    "                    \n",
    "                    fb = getAllComments(post['id'],nb_comments_per_post,serie,fb)\n",
    "                                                     \n",
    "                except IndexError or KeyError:\n",
    "                    continue \n",
    "            except KeyError:\n",
    "                continue       \n",
    "        try:\n",
    "            url = posts['paging']['next']\n",
    "            #print('next')\n",
    "        except KeyError:\n",
    "            #print('no next')\n",
    "            break\n",
    "            \n",
    "    fb['comment message'] = fb['comment message'].astype(str)\n",
    "    fb['post message'] = fb['post message'].astype(str)\n",
    "    fb['comment likes']  = fb['comment likes'].astype(int)\n",
    "    fb['post likes']  = fb['post likes'].astype(int)\n",
    "    fb['nb of comments']  = fb['nb of comments'].astype(int)\n",
    "    fb['post time'] = fb['post time'].apply(convert_time)\n",
    "    fb['comment time'] = fb['comment time'].apply(convert_time)\n",
    "\n",
    "    print(\"Number of posts: \",i)\n",
    "    #display(fb[:])\n",
    "    \n",
    "    folder = os.path.join( 'data', 'social_media')\n",
    "    try:\n",
    "        os.makedirs(folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    filename = os.path.join(folder, 'facebook'+ filename +'.sqlite')\n",
    "    fb.to_sql('facebook', 'sqlite:///' + filename, if_exists='replace')\n",
    "        \n",
    "                               \n",
    "                               \n",
    "    return fb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def postReactions (postId,postType,filename):\n",
    "    #fields = 'id,created_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'\n",
    "    \n",
    "    if postType==0:\n",
    "        fields = 'id,updated_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'\n",
    "    else:\n",
    "        fields = 'id,created_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'\n",
    "        \n",
    "    url = 'https://graph.facebook.com/{}?fields={}&access_token={}'.format(postId, fields, token)\n",
    "    \n",
    "    fb = pd.DataFrame(columns=['post message','post id','post time','post likes','nb of comments','comment id', 'comment message', 'comment time', 'comment likes'])# 'user name']) #'age', 'gender','location','political','religion','education'])\n",
    "    serie={'post message':[],'post id':[],'post time':[],'post likes':[],'nb of comments':[],'comment id':[],'comment message':[],'comment time':[], 'comment likes':[]}#,'user name':[]} # 'age':[], 'gender':[],'location':[],'political':[],'religion':[],'education':[]};\n",
    "  \n",
    "    post = requests.get(url).json()\n",
    "    post_error= str(post)\n",
    "    \n",
    "    if post_error.find('Tried accessing nonexisting field (message)')!=-1:\n",
    "        try:\n",
    "            fields=fields.replace('message', 'description')\n",
    "            url = 'https://graph.facebook.com/{}?fields={}&access_token={}'.format(postId, fields, token)\n",
    "            post = requests.get(url).json()\n",
    "        except:\n",
    "            print('')\n",
    "     \n",
    "   \n",
    "    try:\n",
    "        # Only work with posts with text.\n",
    "        if post_error.find('Tried accessing nonexisting field (message)')!=-1:\n",
    "            test = post['description']\n",
    "        else:\n",
    "            test = post['message']\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Only work with posts with comments which have text.\n",
    "            nb_comments_per_post=post['comments']['summary']['total_count']\n",
    "            #print(nb_comments_per_post,' comments')\n",
    "                \n",
    "            x= post['comments']['data'][0]['message'] #IndexError if no comment on the page, only work with posts\n",
    "            # which have at least 1 comment\n",
    "            \n",
    "            if post_error.find('Tried accessing nonexisting field (message)')!=-1:\n",
    "                post_message=post['description'] \n",
    "            else: \n",
    "                post_message=post['message'] \n",
    "            \n",
    "            serie['post message']=post_message\n",
    "            #serie['post time']=post['created_time']\n",
    "            \n",
    "            if postType==0:\n",
    "                serie['post time']=post['updated_time']\n",
    "            else:\n",
    "                serie['post time']=post['created_time']\n",
    "            \n",
    "            serie['post likes']=post['likes']['summary']['total_count']\n",
    "            serie['nb of comments']= post['comments']['summary']['total_count']\n",
    "            serie['post id']=post['id']\n",
    "        \n",
    "            fb = getAllComments(postId,nb_comments_per_post,serie,fb)\n",
    "                                     \n",
    "        except IndexError or KeyError:\n",
    "            print('')\n",
    "    except KeyError:\n",
    "        print('')\n",
    "\n",
    "            \n",
    "    fb['comment message'] = fb['comment message'].astype(str)\n",
    "    fb['post message'] = fb['post message'].astype(str)\n",
    "    fb['comment likes']  = fb['comment likes'].astype(int)\n",
    "    fb['post likes']  = fb['post likes'].astype(int)\n",
    "    fb['nb of comments']  = fb['nb of comments'].astype(int)\n",
    "    fb['post time'] = fb['post time'].apply(convert_time)\n",
    "    fb['comment time'] = fb['comment time'].apply(convert_time)\n",
    "    \n",
    "    #display(fb[:])\n",
    "    \n",
    "    folder = os.path.join( 'data', 'social_media')\n",
    "    try:\n",
    "        os.makedirs(folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    filename = os.path.join(folder, 'facebook'+ filename +'.sqlite')\n",
    "    fb.to_sql('facebook', 'sqlite:///' + filename, if_exists='replace')\n",
    "\n",
    "    \n",
    "    return fb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#function to get the SQLite file\n",
    "def getTable(filename): #Example= 'facebookCNN_Ab1.sqlite'\n",
    "    folder = os.path.join( 'data', 'social_media')\n",
    "    filename = os.path.join(folder, filename)\n",
    "    con = sqlite3.connect(filename)\n",
    "    fb=pd.read_sql(('SELECT * FROM facebook'),con)\n",
    "    return fb\n",
    "\n",
    "#function to get the input of the classifier\n",
    "\n",
    "def getInputComment(fb,ClassifierName):\n",
    "    val=fb['comment message'].values.tolist()\n",
    "    if(ClassifierName=='SAT'):\n",
    "        new_val,f= Transform_To_Input_Format_SAT_Classifiers(val)\n",
    "    if(ClassifierName=='Amazon'):\n",
    "        new_val= Transform_To_Input_Format_Amazon(val)\n",
    "    return new_val\n",
    "\n",
    "def getInputPost(fb,ClassifierName):\n",
    "    val=fb['post message'].values.tolist()\n",
    "    if(ClassifierName=='SAT'):\n",
    "        new_val,f= Transform_To_Input_Format_SAT_Classifiers(val)\n",
    "    if(ClassifierName=='Amazon'):\n",
    "        new_val= Transform_To_Input_Format_Amazon(val)\n",
    "    return new_val\n",
    "\n",
    "# function to get the prediction and add to \n",
    "def getAndAddPrediction(input_comment,input_post,fb,ClassifierName): # example ClassifierName='PAClassifierOnAmazon.pkl'\n",
    "    prediction_comment=joblib.load(ClassifierName)\n",
    "    sentiment_comment=prediction_comment.predict(input_comment)\n",
    "    fb['comment sentiment']=sentiment_comment\n",
    "    \n",
    "    prediction_post=joblib.load(ClassifierName)\n",
    "    sentiment_post=prediction_post.predict(input_post)\n",
    "    fb['post sentiment']=sentiment_post\n",
    "    return \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#BINARY\n",
    "\n",
    "#PAGE\n",
    "#lastPostsReactions(postId,1,'CNN_page')\n",
    "fb_page_b =getTable('facebookCNN_page.sqlite')\n",
    "input_comment=getInputComment(fb_page_b,'SAT')\n",
    "input_post=getInputPost(fb_page_b,'SAT')\n",
    "getAndAddPrediction(input_comment, input_post,fb_page_b,'PAClassifierOnSat.pkl')\n",
    "#display(fb_page_b[:])\n",
    "\n",
    "#POST\n",
    "#postReactions(postId,1,'CNN_Ab1')\n",
    "fb_post_b =getTable('facebookDT3.sqlite')\n",
    "input_comment=getInputComment(fb_post_b,'SAT')\n",
    "input_post=getInputPost(fb_post_b,'SAT')\n",
    "getAndAddPrediction(input_comment, input_post,fb_post_b,'PAClassifierOnSat.pkl')\n",
    "#display(fb_post_b[:])\n",
    "\n",
    "#AMAZON\n",
    "\n",
    "#PAGE\n",
    "#lastPostsReactions(postId,1,'CNN_page')\n",
    "fb_page =getTable('facebookCNN_page.sqlite')\n",
    "input_comment=getInputComment(fb_page,'Amazon')\n",
    "input_post=getInputPost(fb_page,'Amazon')\n",
    "getAndAddPrediction(input_comment, input_post,fb_page,'PAClassifierOnAmazon.pkl')\n",
    "#display(fb_page[:])\n",
    "\n",
    "#POST\n",
    "#postReactions(postId,1,'CNN_Ab1')\n",
    "fb_post =getTable('facebookDT3.sqlite')\n",
    "input_comment=getInputComment(fb_post,'Amazon')\n",
    "input_post=getInputPost(fb_post,'Amazon')\n",
    "getAndAddPrediction(input_comment, input_post,fb_post,'PAClassifierOnAmazon.pkl')\n",
    "#display(fb_post[:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for a post analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dateutil.parser as dateparser\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Give the audience response to the post in function of the time of the comments\n",
    "#Plot an histogram\n",
    "\n",
    "def SentimentVSTimefForAPost(fb,sentiment_type,bins): #sentiment_type = 'binary' or 'five'\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    \n",
    "    sent =fb['comment sentiment']\n",
    "    \n",
    "    h=fb['comment time']\n",
    "    pm=fb['post message']\n",
    "    pm=pm[1]\n",
    "    \n",
    "    pt=fb['post time']\n",
    "    pt=pt[1]\n",
    "    \n",
    "    postId=fb['post id']\n",
    "    postId=postId[1]\n",
    "\n",
    "    t=[]\n",
    "    for item in h:\n",
    "        t.append(matplotlib.dates.date2num(dateparser.parse(item)))\n",
    "        \n",
    "    mu=np.mean(t)\n",
    "    sigma=np.std(t)\n",
    "    \n",
    "    t2=[]\n",
    "    sent2=[]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Number of comments')\n",
    "    \n",
    "    val0=[]\n",
    "    val1=[]\n",
    "    \n",
    "    for i, item in enumerate(t):\n",
    "        if np.abs((item-mu)/sigma)<10:\n",
    "            t2.append(item)\n",
    "            sent2.append(sent[i])\n",
    "    \n",
    "    if(sentiment_type=='five'):\n",
    "        val2=[]\n",
    "        val3=[]\n",
    "        val4=[]\n",
    "        for i in range(0,len(sent2)-1):\n",
    "            if sent2[i]==1:\n",
    "                val0.append(t2[i])\n",
    "            if sent2[i]==2:\n",
    "                val1.append(t2[i])\n",
    "            if sent2[i]==3:\n",
    "                val2.append(t2[i])\n",
    "            if sent2[i]==4:\n",
    "                val3.append(t2[i])\n",
    "            if sent2[i]==5:\n",
    "                val4.append(t2[i])\n",
    "\n",
    "\n",
    "    \n",
    "        ax.hist([val0,val1,val2,val3,val4],log=True,color=c,label=['1=negative','2','3','4','5=positive'],bins=bins)\n",
    "        \n",
    "    if(sentiment_type=='binary'):\n",
    "        for i in range(0,len(sent2)-1):\n",
    "            if sent2[i]=='0':\n",
    "                val0.append(t2[i])\n",
    "            if sent2[i]=='1':\n",
    "                val1.append(t2[i])  \n",
    "        ax.hist([val0,val1],log=True,color=c_b,label=['0=negative','1=positive'], bins=bins)\n",
    "\n",
    "    #draw a red line at the time of the post\n",
    "    matplotlib.pyplot.axvline(x=matplotlib.dates.date2num(dateparser.parse(pt)),color='blue')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    locator = matplotlib.dates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(matplotlib.dates.AutoDateFormatter(locator))\n",
    "    fig.autofmt_xdate()\n",
    "    matplotlib.pyplot.title('Sentiment in function of the time')\n",
    "    plt.ylim(bottom=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    return\n",
    "\n",
    "# Give the correspondance between likes and the sentiment value of the comments\n",
    "#plot number of comments VS numbers of likes on the comment with the sentimal value as a parameter \n",
    "def SentimentVSCommentsLikes(fb, sentiment_type):\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    likes=fb['comment likes']\n",
    "    \n",
    "    postId=fb['post id']\n",
    "    postId=postId[1]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "   \n",
    "    likes0=[]\n",
    "    likes1=[]\n",
    "    \n",
    "    if(sentiment_type=='five'):\n",
    "        likes2=[]\n",
    "        likes3=[]\n",
    "        likes4=[]\n",
    "        for i in range(0,len(fb)-1):\n",
    "            if fb.ix[i]['comment sentiment']==1:\n",
    "                likes0.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']==2:\n",
    "                likes1.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']==3:\n",
    "                likes2.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']==4:\n",
    "                likes3.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']==5:\n",
    "                likes4.append(likes[i])\n",
    "\n",
    "        \n",
    "        max_likes=max(likes)\n",
    "        max_likes=np.ceil(np.log2(max_likes))\n",
    "        \n",
    "        cnt0=[]\n",
    "        cnt1=[]\n",
    "        cnt2=[]\n",
    "        cnt3=[]\n",
    "        cnt4=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        cnt0.append((likes0==np.float64(0)).sum())\n",
    "        cnt1.append((likes1==np.float64(0)).sum())\n",
    "        cnt2.append((likes2==np.float64(0)).sum())\n",
    "        cnt3.append((likes3==np.float64(0)).sum())\n",
    "        cnt4.append((likes4==np.float64(0)).sum())\n",
    "        \n",
    "        \n",
    "        title.append('0')\n",
    "        \n",
    "        for i in np.arange(1,max_likes+1):\n",
    "            cnt0.append(((likes0>=np.power(2,i-1))&(likes0<np.power(2,i))).sum())\n",
    "            cnt1.append(((likes1>=np.power(2,i-1))&(likes1<np.power(2,i))).sum())\n",
    "            cnt2.append(((likes2>=np.power(2,i-1))&(likes2<np.power(2,i))).sum())\n",
    "            cnt3.append(((likes3>=np.power(2,i-1))&(likes3<np.power(2,i))).sum())\n",
    "            cnt4.append(((likes4>=np.power(2,i-1))&(likes4<np.power(2,i))).sum())\n",
    "            \n",
    "            if i==1:\n",
    "                title.append('1')\n",
    "            else:\n",
    "                title.append((np.power(2,i-1)).astype(int).astype('str')+ ' to ' + (np.power(2,i)-1).astype(int).astype('str'))\n",
    "        \n",
    "        \n",
    "        #ax.hist([likes0,likes1,likes2,likes3,likes4],log=True,color=c,label=['1=negative','2','3','4','5=positive'],bins=[-0.5,0.5,2.5,6.5,14.5,30.5,62.5,126.5,254.5,510.5,1022.5])\n",
    "       \n",
    "        delay=0.7\n",
    "        label_five=['1=negative','2','3','4','5=positive']\n",
    "        r=np.arange(max_likes+1)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt0,width=delay*0.2,color=c[0],log=True,label=label_five[0])\n",
    "        ax.bar(r+delay*0.2+(1-delay)/2,cnt1,width=delay*0.2,color=c[1],log=True,label=label_five[1])\n",
    "        ax.bar(r+delay*0.4+(1-delay)/2,cnt2,width=delay*0.2,color=c[2],log=True,label=label_five[2])\n",
    "        ax.bar(r+delay*0.6+(1-delay)/2,cnt3,width=delay*0.2,color=c[3],log=True,label=label_five[3])\n",
    "        ax.bar(r+delay*0.8+(1-delay)/2,cnt4,width=delay*0.2,color=c[4],log=True,label=label_five[4])\n",
    "        \n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "        \n",
    "        \n",
    "    if(sentiment_type=='binary'):\n",
    "        for i in range(0,len(fb)-1):\n",
    "            if fb.ix[i]['comment sentiment']=='0':\n",
    "                likes0.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']=='1':\n",
    "                likes1.append(likes[i])\n",
    "        \n",
    "        max_likes=max(likes)\n",
    "        max_likes=np.ceil(np.log2(max_likes))\n",
    "        \n",
    "        cnt0=[]\n",
    "        cnt1=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        cnt0.append((likes0==np.float64(0)).sum())\n",
    "        cnt1.append((likes1==np.float64(0)).sum())\n",
    "                \n",
    "        title.append('0')\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_likes+1):\n",
    "            cnt0.append(((likes0>=np.power(2,i-1))&(likes0<np.power(2,i))).sum())\n",
    "            cnt1.append(((likes1>=np.power(2,i-1))&(likes1<np.power(2,i))).sum())\n",
    "            \n",
    "            if i==1:\n",
    "                title.append('1')\n",
    "            else:\n",
    "                title.append((np.power(2,i-1)).astype(int).astype('str')+ ' to ' + (np.power(2,i)-1).astype(int).astype('str'))\n",
    "\n",
    "        label_bin=['0=negative','1=positive']\n",
    "        \n",
    "        delay=0.7\n",
    "        r=np.arange(max_likes+1)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt0,width=delay*0.5,color=c_b[0],log=True,label=label_bin[0])\n",
    "        ax.bar(r+delay*0.5+(1-delay)/2,cnt1,width=delay*0.5,color=c_b[1],log=True,label=label_bin[1])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "        \n",
    "    \n",
    "    plt.xlabel('Number of likes')\n",
    "    plt.ylabel('Number of comments')\n",
    "    matplotlib.pyplot.title('Sentiment in function of the number of likes')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    plt.ylim(bottom=0.1)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# function for :  Sentiment evaluation over all comments of a post  (not in fucntion of the time)\n",
    "def ReactionToAPost(fb,sentiment_type):\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    mes=fb['comment message']\n",
    "    pm=fb['post message']\n",
    "    pm=pm[1]\n",
    "\n",
    "    \n",
    "    postId=fb['post id']\n",
    "    postId=postId[1]\n",
    "\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    plt.xlabel('Sentiment Value')\n",
    "    plt.ylabel('Number of comments')\n",
    "    \n",
    "    val0=[]\n",
    "    val1=[]\n",
    "    \n",
    "    if(sentiment_type=='five'):\n",
    "        val2=[]\n",
    "        val3=[]\n",
    "        val4=[]\n",
    "        for i in range(0,len(fb)-1):\n",
    "            if fb.ix[i]['comment sentiment']==1:\n",
    "                val0.append(1)\n",
    "            if fb.ix[i]['comment sentiment']==2:\n",
    "                val1.append(2)\n",
    "            if fb.ix[i]['comment sentiment']==3:\n",
    "                val2.append(3)\n",
    "            if fb.ix[i]['comment sentiment']==4:\n",
    "                val3.append(4)\n",
    "            if fb.ix[i]['comment sentiment']==5:\n",
    "                val4.append(5)\n",
    "            \n",
    "        \n",
    "        delay=0.7\n",
    "        \n",
    "        ax.bar(np.arange((1-delay)/2,5+(1-delay)/2,1),[len(val0),len(val1),len(val2),len(val3),len(val4)],width=delay,color=c)\n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator([0.5,1.5,2.5,3.5,4.5]))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((['1','2','3','4','5'])))\n",
    "        \n",
    "    if(sentiment_type=='binary'):\n",
    "        for i in range(0,len(fb)-1):\n",
    "            if fb.ix[i]['comment sentiment']=='0':\n",
    "                val0.append(0)\n",
    "            if fb.ix[i]['comment sentiment']=='1':\n",
    "                val1.append(1) \n",
    "        \n",
    "        delay=0.7\n",
    "        \n",
    "        ax.bar([(1-delay)/2,1+(1-delay)/2],[len(val0),len(val1)],width=delay,color=c_b)\n",
    "        ax.set_xticks=([0.5,1.5])\n",
    "        ax.set_xticklabels(('','0=negative','','1=positive'))\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    matplotlib.pyplot.title('Number of comment expressing each sentiment')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test\n",
    "SentimentVSTimefForAPost(fb_post_b,'binary',50)\n",
    "SentimentVSCommentsLikes(fb_post_b,'binary')\n",
    "ReactionToAPost(fb_post_b,'binary')\n",
    "post_sentiment=fb_post_b['post sentiment']\n",
    "post_sentiment=post_sentiment[1]\n",
    "print(\"Post sentiment: \", post_sentiment)\n",
    "\n",
    "#test\n",
    "SentimentVSTimefForAPost(fb_post,'five',50)\n",
    "SentimentVSCommentsLikes(fb_post,'five')\n",
    "ReactionToAPost(fb_post,'five')\n",
    "post_sentiment=fb_post['post sentiment']\n",
    "post_sentiment=post_sentiment[1]\n",
    "print(\"Post sentiment: \", post_sentiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for a page analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Functions for  a page analysis\n",
    "\n",
    "# function for :Look the audience response to the posts in function of the time of the post. \n",
    "    \n",
    "# look at the general audience response on the page.\n",
    "\n",
    "import dateutil.parser as dateparser\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# function for :If positive post , more likes ? more comments ? more positives/negatives comments ?. (also histogramme)\n",
    "def PostsSentimentVSPostsLikes(fb,sentiment_type):\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    likes=fb['post likes']\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    \n",
    "    \n",
    "    max_likes=max(likes)\n",
    "    slot_width=np.ceil(max_likes/10)\n",
    "\n",
    "    if (np.modf(np.log10(slot_width))[0]>=0.2) and (np.modf(np.log10(slot_width))[0]<0.5):\n",
    "        slot_width=2*(np.power(10,np.floor(np.log10(slot_width))))\n",
    "    else:\n",
    "        if (np.modf(np.log10(slot_width))[0]>=0.5) and (np.modf(np.log10(slot_width))[0]<0.8):\n",
    "            slot_width=5*(np.power(10,np.trunc(np.log10(slot_width))))\n",
    "        else:\n",
    "            slot_width=(np.power(10,np.round(np.log10(slot_width))))\n",
    "\n",
    "    max_likes=np.ceil(max_likes/slot_width)\n",
    "    \n",
    "\n",
    "    likes1=[]\n",
    "\n",
    "    if(sentiment_type=='five'):\n",
    "        likes2=[]\n",
    "        likes3=[]\n",
    "        likes4=[]\n",
    "        likes5=[]\n",
    "        i=0\n",
    "        x = True\n",
    "        newPostId=''\n",
    "        while(x) :\n",
    "            lastPostId=newPostId\n",
    "            newPostId=fb.ix[i]['post id']\n",
    "                \n",
    "            if fb.ix[i]['post sentiment']==1:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes1.append(likes[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==2:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes2.append(likes[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==3:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes3.append(likes[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==4:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes4.append(likes[i])\n",
    "                i = i + 1 \n",
    "            if fb.ix[i]['post sentiment']==5:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes5.append(likes[i])\n",
    "                i = i + 1\n",
    "            if i == len(fb)-1:\n",
    "                x=False\n",
    "                \n",
    "        cnt1=[]\n",
    "        cnt2=[]\n",
    "        cnt3=[]\n",
    "        cnt4=[]\n",
    "        cnt5=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_likes+1):\n",
    "            cnt1.append(((likes1>=slot_width*(i-1))&(likes1<slot_width*i)).sum())\n",
    "            cnt2.append(((likes2>=slot_width*(i-1))&(likes2<slot_width*i)).sum())\n",
    "            cnt3.append(((likes3>=slot_width*(i-1))&(likes3<slot_width*i)).sum())\n",
    "            cnt4.append(((likes4>=slot_width*(i-1))&(likes4<slot_width*i)).sum())\n",
    "            cnt5.append(((likes5>=slot_width*(i-1))&(likes5<slot_width*i)).sum())\n",
    "            title.append((slot_width*(i-1)).astype(int).astype('str')+ ' to ' + (slot_width*i-1).astype(int).astype('str'))\n",
    "        \n",
    "        delay=0.7\n",
    "        label_five=['1=negative','2','3','4','5=positive']\n",
    "        r=np.arange(max_likes)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt1,width=delay*0.5,color=c[0],label=label_five[0])\n",
    "        ax.bar(r+delay*0.2+(1-delay)/2,cnt2,width=delay*0.2,color=c[1],label=label_five[1])\n",
    "        ax.bar(r+delay*0.4+(1-delay)/2,cnt3,width=delay*0.2,color=c[2],label=label_five[2])\n",
    "        ax.bar(r+delay*0.6+(1-delay)/2,cnt4,width=delay*0.2,color=c[3],label=label_five[3])\n",
    "        ax.bar(r+delay*0.8+(1-delay)/2,cnt5,width=delay*0.2,color=c[4],label=label_five[4])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "        \n",
    "        #ax.hist([likes1,likes2,likes3,likes4,likes5],color=c,label=['1=negative','2','3','4','5=positive'])\n",
    "   \n",
    "    if(sentiment_type=='binary'):\n",
    "        likes0=[]\n",
    "        x=True\n",
    "        i=0\n",
    "        newPostId=''\n",
    "        while x:\n",
    "            lastPostId=newPostId\n",
    "            newPostId=fb.ix[i]['post id']\n",
    "            if fb.ix[i]['post sentiment']=='0':\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes0.append(likes[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']=='1':\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes1.append(likes[i])\n",
    "                i = i + 1\n",
    "            if i == len(fb)-1:\n",
    "                x=False\n",
    "               \n",
    "        cnt0=[]\n",
    "        cnt1=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_likes+1):\n",
    "            cnt0.append(((likes0>=slot_width*(i-1))&(likes0<slot_width*i)).sum())\n",
    "            cnt1.append(((likes1>=slot_width*(i-1))&(likes1<slot_width*i)).sum())\n",
    "            title.append((slot_width*(i-1)).astype(int).astype('str')+ ' to ' + (slot_width*i-1).astype(int).astype('str'))\n",
    "\n",
    "        label_bin=['0=negative','1=positive']\n",
    "                \n",
    "        #ax.hist([likes0,likes1],color=c_b,label=['0=negative','1=positive'])\n",
    "        \n",
    "        delay=0.7\n",
    "        \n",
    "        r=np.arange(max_likes)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt0,width=delay*0.5,color=c_b[0],label=label_bin[0])\n",
    "        ax.bar(r+delay*0.5+(1-delay)/2,cnt1,width=delay*0.5,color=c_b[1],label=label_bin[1])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "\n",
    "\n",
    "    \n",
    "    plt.xlabel('Number of likes')\n",
    "    plt.ylabel('Number of posts')\n",
    "    matplotlib.pyplot.title('Posts sentiment VS Posts likes')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def PostsSentimentsVSCommentsCount(fb,sentiment_type):\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    count=fb['nb of comments']\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    \n",
    "    \n",
    "    max_count=max(count)\n",
    "    slot_width=np.ceil(max_count/10)\n",
    "\n",
    "    if (np.modf(np.log10(slot_width))[0]>=0.2) and (np.modf(np.log10(slot_width))[0]<0.5):\n",
    "        slot_width=2*(np.power(10,np.floor(np.log10(slot_width))))\n",
    "    else:\n",
    "        if (np.modf(np.log10(slot_width))[0]>=0.5) and (np.modf(np.log10(slot_width))[0]<0.8):\n",
    "            slot_width=5*(np.power(10,np.trunc(np.log10(slot_width))))\n",
    "        else:\n",
    "            slot_width=(np.power(10,np.round(np.log10(slot_width))))\n",
    "\n",
    "    max_count=np.ceil(max_count/slot_width)\n",
    "    \n",
    "   \n",
    "    count1=[]\n",
    "\n",
    "    if(sentiment_type=='five'):\n",
    "        count2=[]\n",
    "        count3=[]\n",
    "        count4=[]\n",
    "        count5=[]\n",
    "        \n",
    "        i=0\n",
    "        x = True\n",
    "        newPostId=''\n",
    "        while(x) :\n",
    "            lastPostId=newPostId\n",
    "            newPostId=fb.ix[i]['post id']\n",
    "                \n",
    "            if fb.ix[i]['post sentiment']==1:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count1.append(count[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==2:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count2.append(count[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==3:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count3.append(count[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==4:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count4.append(count[i])\n",
    "                i = i + 1 \n",
    "            if fb.ix[i]['post sentiment']==5:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count5.append(count[i])\n",
    "                i = i + 1\n",
    "            if i == len(fb)-1:\n",
    "                x=False\n",
    "                \n",
    "        cnt1=[]\n",
    "        cnt2=[]\n",
    "        cnt3=[]\n",
    "        cnt4=[]\n",
    "        cnt5=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_count+1):\n",
    "            cnt1.append(((count1>=slot_width*(i-1))&(count1<slot_width*i)).sum())\n",
    "            cnt2.append(((count2>=slot_width*(i-1))&(count2<slot_width*i)).sum())\n",
    "            cnt3.append(((count3>=slot_width*(i-1))&(count3<slot_width*i)).sum())\n",
    "            cnt4.append(((count4>=slot_width*(i-1))&(count4<slot_width*i)).sum())\n",
    "            cnt5.append(((count5>=slot_width*(i-1))&(count5<slot_width*i)).sum())\n",
    "            title.append((slot_width*(i-1)).astype(int).astype('str')+ ' to ' + (slot_width*i-1).astype(int).astype('str'))\n",
    "        \n",
    "        delay=0.7\n",
    "        label_five=['1=negative','2','3','4','5=positive']\n",
    "        \n",
    "        r=np.arange(max_count)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt1,width=delay*0.5,color=c[0],label=label_five[0])\n",
    "        ax.bar(r+delay*0.2+(1-delay)/2,cnt2,width=delay*0.2,color=c[1],label=label_five[1])\n",
    "        ax.bar(r+delay*0.4+(1-delay)/2,cnt3,width=delay*0.2,color=c[2],label=label_five[2])\n",
    "        ax.bar(r+delay*0.6+(1-delay)/2,cnt4,width=delay*0.2,color=c[3],label=label_five[3])\n",
    "        ax.bar(r+delay*0.8+(1-delay)/2,cnt5,width=delay*0.2,color=c[4],label=label_five[4])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "                \n",
    "        \n",
    "        #ax.hist([count1,count2,count3,count4,count5],color=c,label=['1=negative','2','3','4','5=positive'])\n",
    "   \n",
    "    if(sentiment_type=='binary'):\n",
    "        count0=[]\n",
    "        x=True\n",
    "        i=0\n",
    "        newPostId=''\n",
    "        while x:\n",
    "            lastPostId=newPostId\n",
    "            newPostId=fb.ix[i]['post id']\n",
    "            if fb.ix[i]['post sentiment']=='0':\n",
    "                if(lastPostId != newPostId):\n",
    "                    count0.append(count[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']=='1':\n",
    "                if(lastPostId != newPostId):\n",
    "                    count1.append(count[i])\n",
    "                i = i + 1\n",
    "            if i == len(fb)-1:\n",
    "                x=False\n",
    "                \n",
    "        cnt0=[]\n",
    "        cnt1=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_count+1):\n",
    "            cnt0.append(((count0>=slot_width*(i-1))&(count0<slot_width*i)).sum())\n",
    "            cnt1.append(((count1>=slot_width*(i-1))&(count1<slot_width*i)).sum())\n",
    "            title.append((slot_width*(i-1)).astype(int).astype('str')+ ' to ' + (slot_width*i-1).astype(int).astype('str'))\n",
    "\n",
    "        label_bin=['0=negative','1=positive']\n",
    "                \n",
    "        #ax.hist([likes0,likes1],color=c_b,label=['0=negative','1=positive'])\n",
    "        \n",
    "        delay=0.7\n",
    "        \n",
    "        r=np.arange(max_count)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt0,width=delay*0.5,color=c_b[0],label=label_bin[0])\n",
    "        ax.bar(r+delay*0.5+(1-delay)/2,cnt1,width=delay*0.5,color=c_b[1],label=label_bin[1])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "                \n",
    "        #ax.hist([count0,count1],color=c_b,label=['0=negative','1=positive'])\n",
    "\n",
    "\n",
    "    \n",
    "    plt.xlabel('Number of comments')\n",
    "    plt.ylabel('Number of posts')\n",
    "    matplotlib.pyplot.title('Post sentiment VS Numbers of comments ')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    plt.show()\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test Binary\n",
    "PostsSentimentVSPostsLikes(fb_page_b,'binary')\n",
    "PostsSentimentsVSCommentsCount(fb_page_b,'binary')\n",
    "\n",
    "#test AMAZON\n",
    "PostsSentimentVSPostsLikes(fb_page,'five')\n",
    "PostsSentimentsVSCommentsCount(fb_page,'five')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllGraphsForPost(filename,n):\n",
    "\n",
    "    #Binary\n",
    "    fb_post_b =getTable(filename)\n",
    "    input_comment=getInputComment(fb_post_b,'SAT')\n",
    "    input_post=getInputPost(fb_post_b,'SAT')\n",
    "    getAndAddPrediction(input_comment, input_post,fb_post_b,'PAClassifierOnSat.pkl')\n",
    "\n",
    "\n",
    "    fb_post =getTable(filename)\n",
    "    input_comment=getInputComment(fb_post,'Amazon')\n",
    "    input_post=getInputPost(fb_post,'Amazon')\n",
    "    getAndAddPrediction(input_comment, input_post,fb_post,'PAClassifierOnAmazon.pkl')\n",
    "\n",
    "    SentimentVSTimefForAPost(fb_post_b,'binary',n)\n",
    "    SentimentVSCommentsLikes(fb_post_b,'binary')\n",
    "    ReactionToAPost(fb_post_b,'binary')\n",
    "    post_sentiment=fb_post_b['post sentiment']\n",
    "    post_sentiment=post_sentiment[1]\n",
    "    print(\"Post sentiment: \", post_sentiment)\n",
    "\n",
    "\n",
    "    SentimentVSTimefForAPost(fb_post,'five',n)\n",
    "    SentimentVSCommentsLikes(fb_post,'five')\n",
    "    ReactionToAPost(fb_post,'five')\n",
    "    post_sentiment=fb_post['post sentiment']\n",
    "    post_sentiment=post_sentiment[1]\n",
    "    print(\"Post sentiment: \", post_sentiment)\n",
    "    return\n",
    "\n",
    "def getAllGraphsForPage(filename,n):\n",
    "    \n",
    "    #BINARY\n",
    "\n",
    "    fb_page_b =getTable(filename)\n",
    "    input_comment=getInputComment(fb_page_b,'SAT')\n",
    "    input_post=getInputPost(fb_page_b,'SAT')\n",
    "    getAndAddPrediction(input_comment, input_post,fb_page_b,'PAClassifierOnSat.pkl')\n",
    "    PostsSentimentVSPostsLikes(fb_page_b,'binary')\n",
    "    PostsSentimentsVSCommentsCount(fb_page_b,'binary')\n",
    "\n",
    "    #AMAZON\n",
    "\n",
    "    fb_page =getTable(filename)\n",
    "    input_comment=getInputComment(fb_page,'Amazon')\n",
    "    input_post=getInputPost(fb_page,'Amazon')\n",
    "    getAndAddPrediction(input_comment, input_post,fb_page,'PAClassifierOnAmazon.pkl')\n",
    "    PostsSentimentVSPostsLikes(fb_page,'five')\n",
    "    PostsSentimentsVSCommentsCount(fb_page,'five')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on real data\n",
    "\n",
    "## Analysis  on posts\n",
    "\n",
    "### Theme : Terrorism\n",
    "#### The Orlando shooting\n",
    "\n",
    "CNN article about orlando shooting's victims suing twitter and facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CNN article about orlando shooting's victims suing twitter and facebook\n",
    "\n",
    "#page='cnninternational'\n",
    "#postId= '10154812470324641'\n",
    "#fb= postReactions(postId,1,'CNN_OS1')\n",
    "filename='facebookCNN_OS1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN article about John McCain saying that Obama is responsible for the Orlando shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154219265779641'\n",
    "#fb= postReactions(postId,1,'CNN_OS2')\n",
    "filename='facebookCNN_OS2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN articles about gun sales after Orlando shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154210843679641'\n",
    "#fb= postReactions(postId,1,'CNN_OS3')\n",
    "filename='facebookCNN_OS3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme : Aborption\n",
    "CNN articles about Lena Dunham's aborption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154817313449641'\n",
    "#fb= postReactions(postId,1,'CNN_Ab1')\n",
    "filename='facebookCNN_Ab1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN article about Texas fetus burial obliagtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154738564069641'\n",
    "#fb= postReactions(postId,1,'CNN_Ab2')\n",
    "filename='facebookCNN_Ab2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN article about pope abortion forgivness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154708071624641'\n",
    "#fb= postReactions(postId,1,'CNN_Ab3')\n",
    "filename='facebookCNN_Ab3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme : Political\n",
    "#### Brexit\n",
    "\n",
    "David Cameron before brexit speaking about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page = 'DavidCameronOfficial'\n",
    "#postId= '1216426805048302'\n",
    "#fb= postReactions(postId,1,'DC1')\n",
    "filename='facebookDC1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David Cameron 'thank you' note to the voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '1218229621534687'\n",
    "#fb= postReactions(postId,0,'DC2')\n",
    "filename='facebookDC2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### US presidential election"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hillary Clinton asking for votes (before election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page = 'hillaryclinton'\n",
    "#postId= '1322830897773436'\n",
    "#fb= postReactions(postId,1,'HC3')\n",
    "filename='facebookHC3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hillary Clinton defeat speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '1324421317614394'\n",
    "#fb= postReactions(postId,0,'HC2')\n",
    "filename='facebookHC2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Trump about winning more votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page = 'DonaldTrump'\n",
    "#postId= '10158135498775725'\n",
    "#fb= postReactions(postId,0,'DT5')\n",
    "filename='facebookDT5.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Trump speaks about the defeat of the democrats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page = 'DonaldTrump'\n",
    "#postId= '10158423167515725'\n",
    "#fb= postReactions(postId,0,'DT3')\n",
    "filename='facebookDT3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donald Trump\n",
    "Donald Trump reaction to Meryl Streep speach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = 'DonaldTrump'\n",
    "postId= '10158439095420725'\n",
    "fb= postReactions(postId,0,'DT1')\n",
    "filename='facebookDT1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Trump speaks about a good relationship with Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10158430269170725'\n",
    "#fb= postReactions(postId,0,'DT2')\n",
    "filename='facebookDT2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Trump speaks about Obamacare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10158417912085725'\n",
    "#fb= postReactions(postId,0,'DT4')\n",
    "filename='facebookDT4.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme : Environmental\n",
    "#### Dakota Pipeline\n",
    "\n",
    "AJ+ post on Native Americans fighting against the Dakota Access pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page='ajplusenglish'\n",
    "#postId='843345759140266'\n",
    "#fb= postReactions(postId,1,'ENV1')\n",
    "filename='facebookENV1.sqlite'\n",
    "getAllGraphsForPost(filename,50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AJ+ post on celebrations after the U.S. Army Corp of Engineers refused to grant \n",
    "an easement allowing the Dakota Access Pipeline to go under Lake Oahe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId='852012618273580'\n",
    "#fb= postReactions(postId,0,'ENV2')\n",
    "filename='facebookENV2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donald Trump and environmental issues\n",
    "AJ+ post on the hackers that are preserving environmental data before Trump takes office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId='864548553686653'\n",
    "#fb= postReactions(postId,1,'ENV3')\n",
    "filename='facebookENV3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AJ+ post on Donald Trump being picked for the Environmental Protection Agency not believing in man-made climate change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId='855199717954870'\n",
    "#fb= postReactions(postId,1,'ENV4')\n",
    "filename='facebookENV4.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme : Homosexuality\n",
    "\n",
    "CNN post on Pope saying Christians should apologize to gay people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page='cnninternational'\n",
    "#postId='10154249259664641'\n",
    "#fb= postReactions(postId,1,'HM1')\n",
    "filename='facebookHM1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN post on  the fact that more than half of British Muslims think homosexuality should be illegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId='10154056181289641'\n",
    "#fb= postReactions(postId,1,'HM2')\n",
    "#display(fb[:])\n",
    "filename='facebookHM2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis  on pages\n",
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page='cnninternational'\n",
    "lastPostsReactions(page,'CNN',20)\n",
    "filename='facebookCNN.sqlite'\n",
    "def getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donald Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page='DonaldTrump'\n",
    "lastPostsReactions(page,'DT',20)\n",
    "filename='facebookDT.sqlite'\n",
    "def getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hillary Clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = 'hillaryclinton'\n",
    "lastPostsReactions(page,'HC',20)\n",
    "filename='facebookHC.sqlite'\n",
    "def getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark Zuckerberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page='zuck'\n",
    "lastPostsReactions(page,'zuck',20)\n",
    "filename='facebooZUCK.sqlite'\n",
    "def getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9GAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page='9gag'\n",
    "lastPostsReactions(page,'9GAG',20)\n",
    "filename='faceboo9GAG.sqlite'\n",
    "def getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Love What Really Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page='lovewhatreallymatters'\n",
    "lastPostsReactions(page,'LOVE',20)\n",
    "filename='facebooLOVE.sqlite'\n",
    "def getAllGraphsForPage(filename,50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
