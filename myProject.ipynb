{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import scipy.io\n",
    "import csv\n",
    "from IPython.display import display\n",
    "from scipy import sparse\n",
    "import os.path\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier,SGDRegressor,PassiveAggressiveRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB,MultinomialNB\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"stopwords\") \n",
    "nltk.download('punkt')\n",
    "#removing stopwords\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "## drawing tools\n",
    "# Load libraries\n",
    "\n",
    "# Math\n",
    "import numpy as np\n",
    "\n",
    "# Visualization \n",
    "%matplotlib notebook \n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy import ndimage\n",
    "\n",
    "# High-res visualization (but no rotation possible)\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png2x','pdf')\n",
    "\n",
    "# Print output of LFR code\n",
    "import subprocess\n",
    "\n",
    "# Sparse matrix\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "\n",
    "# 3D visualization\n",
    "import pylab\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Import data\n",
    "import scipy.io\n",
    "\n",
    "# Import functions in lib folder\n",
    "import sys\n",
    "sys.path.insert(1, 'lib')\n",
    "\n",
    "# Import helper functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import distance function\n",
    "import sklearn.metrics.pairwise\n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_between_r( s, first, last ):\n",
    "    try:\n",
    "        start = s.rindex( first ) + len( first )\n",
    "        end = s.rindex( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Sentimental Analysis Text dataset: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SAT_data =[]\n",
    "import codecs\n",
    "#with open('Projectdataset/Sentiment Analysis Dataset.csv', 'r') as csvfile:\n",
    "with codecs.open('Projectdataset/Sentiment Analysis Dataset.csv','r',encoding='utf8') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',')#, quotechar='|')\n",
    "    for row in spamreader:\n",
    "        SAT_data.append(row) \n",
    "\n",
    "SAT_data = SAT_data[2:]\n",
    "SAT_data = [[row[1],row[3]] for row in SAT_data]\n",
    "SAT_data = np.array(SAT_data)\n",
    "#SAT_data[:,0] = [int(float(x))*5 for x in SAT_data[:,0]]\n",
    "print('Length = {} '.format(len(SAT_data)))\n",
    "print('Type = {}'.format(type(SAT_data)))\n",
    "print(SAT_data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Amazon dataset: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Amazon_data = []\n",
    "file4 = open(\"Projectdataset/amazonMP3reviews/amazon_mp3\", \"r\")\n",
    "data4 = file4.read()\n",
    "\n",
    "data4 = data4.split(\"#####\")\n",
    "for i in range(1,len(data4)):\n",
    "    text = find_between_r( data4[i], \"[fullText]:\", \"[rating]\" )\n",
    "    text = text.replace(\"\\n\",'')\n",
    "    rating = find_between_r( data4[i],\"[rating]:\",\"[recommend]\")\n",
    "    Amazon_data.append([float(rating),text])\n",
    "Amazon_data = np.asarray(Amazon_data)\n",
    "Amazon_data_chunk = Amazon_data[0:8000]\n",
    "Amazon_data[:,0] = [int(float(x)) for x in Amazon_data[:,0]]\n",
    "print('Length = {} '.format(len(Amazon_data)))\n",
    "print('Type = {}'.format(type(Amazon_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "# Classifier trained on SAT dataset\n",
    "** Feature Extraction\n",
    "** \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "def compute_bag_of_words(text, stopwords , vocab=None):\n",
    "    vectorizer = CountVectorizer(stop_words = stopwords,vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(text)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    return vectors, vocabulary\n",
    "\n",
    "\n",
    "\n",
    "text_sat = column(SAT_data,1)\n",
    "Y_sat = np.asarray(column(SAT_data,0))\n",
    "  \n",
    "bow, vocab = compute_bag_of_words(text_sat, stopwords.words())\n",
    "#KBestModel = SelectKBest(chi2, k=1000).fit(bow, Y_sat)  \n",
    "#indices = KBestModel.get_support(True)\n",
    "#bow_transformed = KBestModel.transform(bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "KBestModel = SelectKBest(chi2, k=1000).fit(bow, Y_sat)  \n",
    "indices = KBestModel.get_support(True)\n",
    "bow_transformed = KBestModel.transform(bow)\n",
    "#sving best features\n",
    "print(\"bow = {}\".format(bow.shape))\n",
    "print(\"bow_transformed = {}\".format(bow_transformed.shape))\n",
    "best_features_Sat = np.array(vocab)[indices]\n",
    "print(len(best_features_Sat))\n",
    "\n",
    "file = codecs.open('Best_Features_SAT.txt', 'w',encoding='utf8')\n",
    "for word in best_features_Sat:\n",
    "    file.write(\"%s\\n\" % word)    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input X is list of strings \n",
    "def Transform_To_Input_Format_SAT_Classifiers(X):\n",
    "    with codecs.open('Best_Features_SAT.txt','r',encoding='utf8') as f:\n",
    "        features = f.readlines()\n",
    "    features = [x.strip(\"\\n\") for x in features]\n",
    "    X_transformed,vocab = compute_bag_of_words(X, stopwords.words(),features)\n",
    "    return X_transformed,features\n",
    "#Example how to use\n",
    "Xinput,f = Transform_To_Input_Format_SAT_Classifiers([\"i am feeling terrible today\",\"I don't like this\"])\n",
    "print(Xinput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_stats = {'n_test': 0, 'n_test_pos': 0}\n",
    "def progress(cls_name, stats):\n",
    "    \"\"\"Report progress information, return a string.\"\"\"\n",
    "    duration = time.time() - stats['t0']\n",
    "    s = \"%20s classifier : \\t\" % cls_name\n",
    "    s += \"%(n_train)6d train docs (%(n_train_pos)6d positive) \" % stats\n",
    "    s += \"%(n_test)6d test docs (%(n_test_pos)6d positive) \" % test_stats\n",
    "    s += \"accuracy: %(accuracy).3f \" % stats\n",
    "    s += \"in %.2fs (%5d docs/s)\" % (duration, stats['n_train'] / duration)\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "batches = []\n",
    "y_all = np.unique(Y_sat)\n",
    "\n",
    "\n",
    "minibatch_size = 10000\n",
    "# divide data into batches\n",
    "i = 0\n",
    "start = 0\n",
    "while(start < bow_transformed.shape[0]):\n",
    "    if(start + minibatch_size < bow_transformed.shape[0]):\n",
    "        batch = bow_transformed[start:start+minibatch_size]\n",
    "        batches.append(batch)\n",
    "        start+=minibatch_size\n",
    "    else:\n",
    "        batch = bow_transformed[start:]\n",
    "        batches.append(batch)\n",
    "        start+=minibatch_size\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "#Classifiers      \n",
    "partial_fit_classifiers = {\n",
    "    'SGD': SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "        eta0=0.0, fit_intercept=True, l1_ratio=0,\n",
    "        learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
    "        penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
    "        verbose=0, warm_start=False),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'NB Multinomial': MultinomialNB(alpha=0.01),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(C=1.0, n_iter=50, shuffle=True, \n",
    "                                                      verbose=0, loss='hinge',\n",
    "                                                      warm_start=False),\n",
    "    'NB Bernoulli': BernoulliNB(alpha=0.01),\n",
    "}\n",
    "                   \n",
    "classifier_information = {\n",
    "     'SGD':[],\n",
    "    'Perceptron': [],\n",
    "    'NB Multinomial': [],\n",
    "    'Passive-Aggressive': [],\n",
    "    'NB Bernoulli':[],\n",
    "}       \n",
    "                  \n",
    "\n",
    "\n",
    "number_minibatch = len(batches)\n",
    "total_vect_time = 0.0\n",
    "    \n",
    "print(number_minibatch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FirstBatch = True\n",
    "k = 0;\n",
    "# Main loop : iterate on mini-batches of examples\n",
    "for i in range(number_minibatch):\n",
    "    print(\"Batch number = {}\".format(i))\n",
    "    batch = batches[i]\n",
    "    X = batch\n",
    "    if(k + minibatch_size < len(Y_sat)):\n",
    "        Y = Y_sat[k:k+minibatch_size]\n",
    "    else:\n",
    "        Y = Y_sat[k:]\n",
    "    k = k+minibatch_size\n",
    "    #classifiers\n",
    "    for cls_name, cls in partial_fit_classifiers.items():\n",
    "        #cross_validation\n",
    "        kf = KFold(n_splits = 10)\n",
    "        results =[]\n",
    "        for train_index,test_index in kf.split(X):\n",
    "            X_train,X_test = X[train_index],X[test_index]\n",
    "            Y_train,Y_test = Y[train_index],Y[test_index]\n",
    "            if(FirstBatch):\n",
    "                cls.partial_fit(X_train, Y_train, classes = y_all)\n",
    "            else:\n",
    "                cls.partial_fit(X_train, Y_train)\n",
    "            train_pred = cls.predict(X_test)\n",
    "            results.append(100*sklearn.metrics.accuracy_score(Y_test, train_pred))\n",
    "        classifier_information[cls_name].append(np.mean(np.array(results)))\n",
    "    FirstBatch = False\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(classifier_information['SGD'])\n",
    "#x = column(accuracy_info, 0)\n",
    "#y = column(accuracy_info, 1)\n",
    "#plt.plot(classifier_information['Preception'])\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(partial_fit_classifiers['SGD'], 'SGDClassifierOnSat.pkl')\n",
    "joblib.dump(partial_fit_classifiers['Perceptron'], 'PerceptronClassifierOnSat.pkl')\n",
    "joblib.dump(partial_fit_classifiers['NB Multinomial'], 'NBMClassifierOnSat.pkl')\n",
    "joblib.dump(partial_fit_classifiers['Passive-Aggressive'], 'PAClassifierOnSat.pkl')\n",
    "joblib.dump(partial_fit_classifiers['NB Bernoulli'], 'NBBClassifierOnSat.pkl')\n",
    "\n",
    "x = range(0,158);\n",
    "i = 0\n",
    "fig, axes = plt.subplots(2, 3, squeeze=True, figsize=(15, 8))\n",
    "for item in classifier_information:\n",
    "    z = np.polyfit(x, classifier_information[item], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[i//3,i%3].plot(x,p(x),\"r--\")\n",
    "    axes[i//3,i%3].plot(x,classifier_information[item])\n",
    "    axes[i//3,i%3].set_title(item)\n",
    "    axes[i//3,i%3].set_ylabel(\"accuracy\")\n",
    "    axes[i//3,i%3].set_xlabel(\"training Size\")\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(classifier_information['SGD'],label ='SGD')\n",
    "plt.plot(classifier_information['Perceptron'],label = 'Perception')\n",
    "plt.plot(classifier_information['NB Multinomial'],label='NB Multinomial')\n",
    "plt.plot(classifier_information['Passive-Aggressive'],label='Passive-Aggressive')\n",
    "plt.plot(classifier_information['NB Bernoulli'],label='NB Bernoulli')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "#plt.legend(['SGD','Perceptron','NB Multinomial','Passive-Aggressive','NB Bernoulli'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier For Amazon data \n",
    "** Feature extraction **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_bag_of_words(text, stopwords, vocab=None):\n",
    "    vectorizer = CountVectorizer(stop_words = stopwords,vocabulary=vocab)\n",
    "    vectors = vectorizer.fit_transform(text)\n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    return vectors, vocabulary\n",
    "\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "\n",
    "text_amazon = column(Amazon_data,1)\n",
    "Y_amazon = column(Amazon_data,0)\n",
    "\n",
    "\n",
    "bow, vocab = compute_bag_of_words(text_amazon, stopwords.words())\n",
    "KBestModel = SelectKBest( k=1000).fit(bow, Y_amazon) \n",
    "indices = KBestModel.get_support(True)\n",
    "bow_transformed = KBestModel.transform(bow)\n",
    "print(\"bow = {}\".format(bow.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "best_features_Amazon = np.array(vocab)[indices]\n",
    "print(len(best_features_Amazon))\n",
    "\n",
    "file = open('Best_Features_Amazon.txt', 'w')\n",
    "for word in best_features_Amazon:\n",
    "    file.write(\"%s\\n\" % word)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input X is list of strings\n",
    "def Transform_To_Input_Format_Amazon(X):\n",
    "    with open('Best_Features_Amazon.txt') as f:\n",
    "        features = f.readlines()\n",
    "    features = [x.strip(\"\\n\") for x in features]\n",
    "    X_transformed,vocab = compute_bag_of_words(X, stopwords.words(),features)\n",
    "    return X_transformed\n",
    "    \n",
    "print(Transform_To_Input_Format_Amazon([\"I am having a wonderfull day\",\"Thank you very much\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Classifier **\n",
    "\n",
    "We train 6 different classifiers on each of the datasets to compare the performance of each classifier:\n",
    "K-NN\n",
    "SVM\n",
    "Random Forest Classifier\n",
    "Ridge Classifier\n",
    "Bernoulli Naive Bayes\n",
    "Mutlinomial Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batches = []\n",
    "Ys=[]\n",
    "Y_amazon = np.array(Y_amazon)\n",
    "y_all = np.unique(np.asarray(column(Amazon_data,0)))\n",
    "print(y_all)\n",
    "minibatch_size = 10000\n",
    "start=0\n",
    "# divide data into batches\n",
    "while(start < bow_transformed.shape[0]):\n",
    "    if(start + minibatch_size < bow_transformed.shape[0]):\n",
    "        batch = bow_transformed[start:start+minibatch_size]\n",
    "        Ys.append(Y_amazon[start:start+minibatch_size])\n",
    "        batches.append(batch)\n",
    "        start+=minibatch_size\n",
    "    else:\n",
    "        batch = bow_transformed[start:]\n",
    "        Ys.append(Y_amazon[start:])\n",
    "        batches.append(batch)\n",
    "        start+=minibatch_size\n",
    "    i += 1\n",
    "\n",
    "        \n",
    "#Classifiers      \n",
    "partial_fit_Regressors = {\n",
    "    'SGD Regressor':SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.001, l1_ratio=0, \n",
    "                                 fit_intercept=True, n_iter=1000, shuffle=True, verbose=0, epsilon=0.01, random_state=None,\n",
    "                                 learning_rate='invscaling', eta0=0.01, power_t=0.25, warm_start=False, average=False),\n",
    "    'Passive-Aggressive Regressor' : PassiveAggressiveRegressor(),\n",
    "     'SGD': SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
    "        eta0=0.0, fit_intercept=True, l1_ratio=0,\n",
    "        learning_rate='optimal', loss='hinge', n_iter=300, n_jobs=1,\n",
    "        penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
    "        verbose=0, warm_start=False),\n",
    "    'Perceptron': Perceptron(penalty='l1', alpha=0.0001, fit_intercept=True, n_iter=50, \n",
    "                             shuffle=True, verbose=0, eta0=1.0, n_jobs=1, random_state=0, \n",
    "                             class_weight=None, warm_start=False),\n",
    "    'NB Multinomial': MultinomialNB(alpha=1),\n",
    "    'Passive-Aggressive': PassiveAggressiveClassifier(),\n",
    "    'NB Bernoulli': BernoulliNB(alpha=0.01),\n",
    "    \n",
    "}\n",
    "Regressors_information = {\n",
    "    'SGD Regressor':[],\n",
    "    'Passive-Aggressive Regressor':[],\n",
    "    'SGD':[],\n",
    "    'Perceptron': [],\n",
    "    'NB Multinomial': [],\n",
    "    'Passive-Aggressive': [],\n",
    "    'NB Bernoulli':[],\n",
    "}   \n",
    "                   \n",
    "cls_stats = {}       \n",
    "                  \n",
    "\n",
    "number_minibatch = len(batches)\n",
    "    \n",
    "print(number_minibatch)\n",
    "FirstBatch = True\n",
    "k = 0\n",
    "# Main loop : iterate on mini-batches of examples\n",
    "for i in range(number_minibatch):\n",
    "    print(\"Batch number = {}\".format(i))\n",
    "    batch = batches[i]\n",
    "    X = batch\n",
    "    Y = Ys[i]\n",
    "    #classifiers\n",
    "    for cls_name, cls in partial_fit_Regressors.items():\n",
    "        tick = time.time()\n",
    "        #cross_validation\n",
    "        kf = KFold(n_splits = 10)\n",
    "        results =[]\n",
    "        for train_index,test_index in kf.split(X):\n",
    "            X_train,X_test = X[train_index],X[test_index]\n",
    "            Y_train,Y_test = Y[train_index],Y[test_index]\n",
    "            if(FirstBatch and cls_name != \"SGD Regressor\" and cls_name != \"Passive-Aggressive Regressor\"):\n",
    "                cls.partial_fit(X_train, Y_train, classes = y_all)\n",
    "                train_pred = cls.predict(X_test)\n",
    "            elif(cls_name != \"SGD Regressor\" and cls_name != \"Passive-Aggressive Regressor\"):\n",
    "                    cls.partial_fit(X_train, Y_train)\n",
    "                    train_pred = cls.predict(X_test)\n",
    "                    results.append(100*sklearn.metrics.accuracy_score(Y_test, train_pred))               \n",
    "            else:\n",
    "                Y_train = [int(x) for x in Y_train]\n",
    "                Y_test = [int(x) for x in Y_test]\n",
    "                cls.partial_fit(X_train, Y_train)\n",
    "                train_pred = cls.predict(X_test)\n",
    "                results.append(sklearn.metrics.mean_squared_error(Y_test, train_pred))\n",
    "        Regressors_information[cls_name].append(np.mean(np.array(results)))\n",
    "    FirstBatch = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Accuracy of Classifiers **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.dump(partial_fit_Regressors['SGD'], 'SGDClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['Perceptron'], 'PerceptronClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['NB Multinomial'], 'NBMClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['Passive-Aggressive'], 'PAClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['NB Bernoulli'], 'NBBClassifierOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['SGD Regressor'], 'SGDRegressorOnAmazon.pkl')\n",
    "joblib.dump(partial_fit_Regressors['Passive-Aggressive Regressor'], 'Passive-AggressiveRegressorOnAmazon.pkl')\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, squeeze=True, figsize=(15, 8))\n",
    "i = 0\n",
    "for item in Regressors_information:\n",
    "    axes[i//4,i%4].plot(Regressors_information[item])\n",
    "    axes[i//4,i%4].set_title(item)\n",
    "    axes[i//4,i%4].set_ylabel(\"accuracy\")\n",
    "    axes[i//4,i%4].set_xlabel(\"training Size\")\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Regressors_information['SGD'],label='SGD')\n",
    "plt.plot(Regressors_information['Passive-Aggressive'],label='Passive-Aggressive')\n",
    "plt.plot(Regressors_information['NB Bernoulli'],label='NB Bernoulli')\n",
    "plt.plot(Regressors_information['Perceptron'],label='Perceptron')\n",
    "plt.plot(Regressors_information['NB Multinomial'],label='NB Multinomial')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Regressors_information['SGD Regressor'],label='SGD Regressor')\n",
    "plt.plot(Regressors_information['Passive-Aggressive Regressor'],label='Passive-Aggressive Regressor')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets from Facebook and Twitter\n",
    "\n",
    "Goal: collect comments from posts on Facebook and Twitter to evaluate the response of the audience (positive,negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "credentials = configparser.ConfigParser()\n",
    "credentials.read('credentials.ini')\n",
    "token = credentials.get('facebook', 'token')\n",
    "\n",
    "import requests  \n",
    "import facebook \n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "\n",
    "#prepare data\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_time(row):\n",
    "    return datetime.strptime(row, '%Y-%m-%dT%H:%M:%S+0000')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAllComments(postId,nb_comments_per_post,serie,fb):\n",
    "    fields_comments = 'comment_count,like_count,created_time,message'\n",
    "    url_comments = 'https://graph.facebook.com/{}/comments/?fields={}&access_token={}'.format(postId, fields_comments, token)\n",
    "    \n",
    "    comment =0\n",
    "    #print(nb_comments_per_post)\n",
    "    while comment <= nb_comments_per_post + 1:\n",
    "    \n",
    "        post_comments=requests.get(url_comments).json()\n",
    "        \n",
    "        \n",
    "        #print('len ',len(post_comments['data']))\n",
    "        i=0\n",
    "        for com in post_comments['data']:\n",
    "            i=i+1\n",
    "           \n",
    "            comment_message=com['message']\n",
    "            serie['comment message'] = comment_message\n",
    "            serie['comment time'] = com['created_time']\n",
    "            serie['comment likes'] =  com['like_count']\n",
    "            serie['comment id']=com['id']\n",
    "\n",
    "            fb = fb.append(serie, ignore_index=True)\n",
    "            comment=comment+1\n",
    "        #print('i',i)\n",
    "        #print(fb['comment message'])\n",
    "        try:\n",
    "            url_comments = post_comments['paging']['next']\n",
    "        except KeyError:\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return fb\n",
    "\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def lastPostsReactions(page,filename,n):\n",
    "    fields = 'id,created_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'\n",
    "    url = 'https://graph.facebook.com/{}/posts?fields={}&access_token={}'.format(page, fields, token)\n",
    "\n",
    "    fb = pd.DataFrame(columns=['post message','post id','post time','post likes','nb of comments','comment id', 'comment message', 'comment time', 'comment likes'])# 'user name']) #'age', 'gender','location','political','religion','education'])\n",
    "    serie={'post message':[],'post id':[],'post time':[],'post likes':[],'nb of comments':[],'comment id':[],'comment message':[],'comment time':[], 'comment likes':[]}#,'user name':[]} # 'age':[], 'gender':[],'location':[],'political':[],'religion':[],'education':[]};\n",
    "    \n",
    "    i=0\n",
    "    while i < n: #len(fb) < n:\n",
    "    \n",
    "        posts = requests.get(url).json()\n",
    "        post_error= str(post)\n",
    "    \n",
    "        if post_error.find('Tried accessing nonexisting field (message)')!=-1:\n",
    "            try:\n",
    "                fields=fields.replace('message', 'description')\n",
    "                url = 'https://graph.facebook.com/{}?fields={}&access_token={}'.format(postId, fields, token)\n",
    "                post = requests.get(url).json()\n",
    "            except:\n",
    "                print('')\n",
    "        \n",
    "        # extract information for each of the received post\n",
    "        for post in posts['data']:\n",
    "           \n",
    "            try:\n",
    "                # Only work with posts with text.\n",
    "                if post_error.find('Tried accessing nonexisting field (message)')!=-1:\n",
    "                    test = post['description']\n",
    "                else:\n",
    "                    test = post['message']\n",
    "               \n",
    "                post_message = test\n",
    "                postId=post['id']\n",
    "                try:\n",
    "                    # Only work with posts with comments which have text.\n",
    "                    nb_comments_per_post=post['comments']['summary']['total_count']\n",
    "                \n",
    "                    x= post['comments']['data'][0]['message'] #IndexError if no comment on the page, only work with posts\n",
    "                    # which have at least 1 comment\n",
    "                    i=i+1\n",
    "    \n",
    "                    serie['post message']=post_message\n",
    "                    serie['post time']=post['created_time']\n",
    "                    serie['post likes']=post['likes']['summary']['total_count']\n",
    "                    serie['nb of comments']= post['comments']['summary']['total_count']\n",
    "                    serie['post id']=post['id']\n",
    "                    \n",
    "                    fb = getAllComments(post['id'],nb_comments_per_post,serie,fb)\n",
    "                                                     \n",
    "                except IndexError or KeyError:\n",
    "                    continue \n",
    "            except KeyError:\n",
    "                continue       \n",
    "        try:\n",
    "            url = posts['paging']['next']\n",
    "            #print('next')\n",
    "        except KeyError:\n",
    "            #print('no next')\n",
    "            break\n",
    "            \n",
    "    fb['comment message'] = fb['comment message'].astype(str)\n",
    "    fb['post message'] = fb['post message'].astype(str)\n",
    "    fb['comment likes']  = fb['comment likes'].astype(int)\n",
    "    fb['post likes']  = fb['post likes'].astype(int)\n",
    "    fb['nb of comments']  = fb['nb of comments'].astype(int)\n",
    "    fb['post time'] = fb['post time'].apply(convert_time)\n",
    "    fb['comment time'] = fb['comment time'].apply(convert_time)\n",
    "\n",
    "    print(\"Number of posts: \",i)\n",
    "    #display(fb[:])\n",
    "    \n",
    "    folder = os.path.join( 'data', 'social_media')\n",
    "    try:\n",
    "        os.makedirs(folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    filename = os.path.join(folder, 'facebook'+ filename +'.sqlite')\n",
    "    fb.to_sql('facebook', 'sqlite:///' + filename, if_exists='replace')\n",
    "        \n",
    "                               \n",
    "                               \n",
    "    return fb\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def postReactions (postId,postType,filename):\n",
    "    #fields = 'id,created_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'\n",
    "    \n",
    "    if postType==0:\n",
    "        fields = 'id,updated_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'\n",
    "    else:\n",
    "        fields = 'id,created_time,message,likes.limit(0).summary(1),comments.limits(0).summary(1)'\n",
    "        \n",
    "    url = 'https://graph.facebook.com/{}?fields={}&access_token={}'.format(postId, fields, token)\n",
    "    \n",
    "    fb = pd.DataFrame(columns=['post message','post id','post time','post likes','nb of comments','comment id', 'comment message', 'comment time', 'comment likes'])# 'user name']) #'age', 'gender','location','political','religion','education'])\n",
    "    serie={'post message':[],'post id':[],'post time':[],'post likes':[],'nb of comments':[],'comment id':[],'comment message':[],'comment time':[], 'comment likes':[]}#,'user name':[]} # 'age':[], 'gender':[],'location':[],'political':[],'religion':[],'education':[]};\n",
    "  \n",
    "    post = requests.get(url).json()\n",
    "    post_error= str(post)\n",
    "    \n",
    "    if post_error.find('Tried accessing nonexisting field (message)')!=-1:\n",
    "        try:\n",
    "            fields=fields.replace('message', 'description')\n",
    "            url = 'https://graph.facebook.com/{}?fields={}&access_token={}'.format(postId, fields, token)\n",
    "            post = requests.get(url).json()\n",
    "        except:\n",
    "            print('')\n",
    "     \n",
    "   \n",
    "    try:\n",
    "        # Only work with posts with text.\n",
    "        if post_error.find('Tried accessing nonexisting field (message)')!=-1:\n",
    "            test = post['description']\n",
    "        else:\n",
    "            test = post['message']\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Only work with posts with comments which have text.\n",
    "            nb_comments_per_post=post['comments']['summary']['total_count']\n",
    "            #print(nb_comments_per_post,' comments')\n",
    "                \n",
    "            x= post['comments']['data'][0]['message'] #IndexError if no comment on the page, only work with posts\n",
    "            # which have at least 1 comment\n",
    "            \n",
    "            if post_error.find('Tried accessing nonexisting field (message)')!=-1:\n",
    "                post_message=post['description'] \n",
    "            else: \n",
    "                post_message=post['message'] \n",
    "            \n",
    "            serie['post message']=post_message\n",
    "            #serie['post time']=post['created_time']\n",
    "            \n",
    "            if postType==0:\n",
    "                serie['post time']=post['updated_time']\n",
    "            else:\n",
    "                serie['post time']=post['created_time']\n",
    "            \n",
    "            serie['post likes']=post['likes']['summary']['total_count']\n",
    "            serie['nb of comments']= post['comments']['summary']['total_count']\n",
    "            serie['post id']=post['id']\n",
    "        \n",
    "            fb = getAllComments(postId,nb_comments_per_post,serie,fb)\n",
    "                                     \n",
    "        except IndexError or KeyError:\n",
    "            print('')\n",
    "    except KeyError:\n",
    "        print('')\n",
    "\n",
    "            \n",
    "    fb['comment message'] = fb['comment message'].astype(str)\n",
    "    fb['post message'] = fb['post message'].astype(str)\n",
    "    fb['comment likes']  = fb['comment likes'].astype(int)\n",
    "    fb['post likes']  = fb['post likes'].astype(int)\n",
    "    fb['nb of comments']  = fb['nb of comments'].astype(int)\n",
    "    fb['post time'] = fb['post time'].apply(convert_time)\n",
    "    fb['comment time'] = fb['comment time'].apply(convert_time)\n",
    "    \n",
    "    #display(fb[:])\n",
    "    \n",
    "    folder = os.path.join( 'data', 'social_media')\n",
    "    try:\n",
    "        os.makedirs(folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    filename = os.path.join(folder, 'facebook'+ filename +'.sqlite')\n",
    "    fb.to_sql('facebook', 'sqlite:///' + filename, if_exists='replace')\n",
    "\n",
    "    \n",
    "    return fb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get predictions"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "#function to get the SQLite file\n",
    "def getTable(filename): #Example= 'facebookCNN_Ab1.sqlite'\n",
    "    folder = os.path.join( 'data', 'social_media')\n",
    "    filename = os.path.join(folder, filename)\n",
    "    con = sqlite3.connect(filename)\n",
    "    fb=pd.read_sql(('SELECT * FROM facebook'),con)\n",
    "    return fb\n",
    "\n",
    "#function to get the input of the classifier\n",
    "\n",
    "def getInputComment(fb,ClassifierName):\n",
    "    val=fb['comment message'].values.tolist()\n",
    "    if(ClassifierName=='SAT'):\n",
    "        new_val,f= Transform_To_Input_Format_SAT_Classifiers(val)\n",
    "    if(ClassifierName=='Amazon'):\n",
    "        new_val= Transform_To_Input_Format_Amazon(val)\n",
    "    return new_val\n",
    "\n",
    "def getInputPost(fb,ClassifierName):\n",
    "    val=fb['post message'].values.tolist()\n",
    "    if(ClassifierName=='SAT'):\n",
    "        new_val,f= Transform_To_Input_Format_SAT_Classifiers(val)\n",
    "    if(ClassifierName=='Amazon'):\n",
    "        new_val= Transform_To_Input_Format_Amazon(val)\n",
    "    return new_val\n",
    "\n",
    "# function to get the prediction and add to \n",
    "def getAndAddPrediction(input_comment,input_post,fb,ClassifierName): # example ClassifierName='PAClassifierOnAmazon.pkl'\n",
    "    prediction_comment=joblib.load(ClassifierName)\n",
    "    sentiment_comment=prediction_comment.predict(input_comment)\n",
    "    fb['comment sentiment']=sentiment_comment\n",
    "    \n",
    "    prediction_post=joblib.load(ClassifierName)\n",
    "    sentiment_post=prediction_post.predict(input_post)\n",
    "    fb['post sentiment']=sentiment_post\n",
    "    return \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 271,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-271-a7e61f6cc5d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mfb_post\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgetTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'facebookDT3.sqlite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0minput_comment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetInputComment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfb_post\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Amazon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0minput_post\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetInputPost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfb_post\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Amazon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mgetAndAddPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_comment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_post\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfb_post\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PAClassifierOnAmazon.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#display(fb_post[:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-270-cbc8438a7844>\u001b[0m in \u001b[0;36mgetInputPost\u001b[0;34m(fb, ClassifierName)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mnew_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTransform_To_Input_Format_SAT_Classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClassifierName\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Amazon'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mnew_val\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTransform_To_Input_Format_Amazon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-230-33dac2edf7cf>\u001b[0m in \u001b[0;36mTransform_To_Input_Format_Amazon\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mX_transformed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_transformed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-228-c3767a4869b1>\u001b[0m in \u001b[0;36mcompute_bag_of_words\u001b[0;34m(text, stopwords, vocab)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_bag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 824\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 241\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#BINARY\n",
    "\n",
    "#PAGE\n",
    "#lastPostsReactions(postId,1,'CNN_page')\n",
    "fb_page_b =getTable('facebookCNN_page.sqlite')\n",
    "input_comment=getInputComment(fb_page_b,'SAT')\n",
    "input_post=getInputPost(fb_page_b,'SAT')\n",
    "getAndAddPrediction(input_comment, input_post,fb_page_b,'PAClassifierOnSat.pkl')\n",
    "#display(fb_page_b[:])\n",
    "\n",
    "#POST\n",
    "#postReactions(postId,1,'CNN_Ab1')\n",
    "fb_post_b =getTable('facebookDT3.sqlite')\n",
    "input_comment=getInputComment(fb_post_b,'SAT')\n",
    "input_post=getInputPost(fb_post_b,'SAT')\n",
    "getAndAddPrediction(input_comment, input_post,fb_post_b,'PAClassifierOnSat.pkl')\n",
    "#display(fb_post_b[:])\n",
    "\n",
    "#AMAZON\n",
    "\n",
    "#PAGE\n",
    "#lastPostsReactions(postId,1,'CNN_page')\n",
    "fb_page =getTable('facebookCNN_page.sqlite')\n",
    "input_comment=getInputComment(fb_page,'Amazon')\n",
    "input_post=getInputPost(fb_page,'Amazon')\n",
    "getAndAddPrediction(input_comment, input_post,fb_page,'PAClassifierOnAmazon.pkl')\n",
    "#display(fb_page[:])\n",
    "\n",
    "#POST\n",
    "#postReactions(postId,1,'CNN_Ab1')\n",
    "fb_post =getTable('facebookDT3.sqlite')\n",
    "input_comment=getInputComment(fb_post,'Amazon')\n",
    "input_post=getInputPost(fb_post,'Amazon')\n",
    "getAndAddPrediction(input_comment, input_post,fb_post,'PAClassifierOnAmazon.pkl')\n",
    "#display(fb_post[:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Analysis "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 272,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for a post analysis"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dateutil.parser as dateparser\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Give the audience response to the post in function of the time of the comments\n",
    "#Plot an histogram\n",
    "\n",
    "def SentimentVSTimefForAPost(fb,sentiment_type,bins): #sentiment_type = 'binary' or 'five'\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    \n",
    "    sent =fb['comment sentiment']\n",
    "    \n",
    "    h=fb['comment time']\n",
    "    pm=fb['post message']\n",
    "    pm=pm[1]\n",
    "    \n",
    "    pt=fb['post time']\n",
    "    pt=pt[1]\n",
    "    \n",
    "    postId=fb['post id']\n",
    "    postId=postId[1]\n",
    "\n",
    "    t=[]\n",
    "    for item in h:\n",
    "        t.append(matplotlib.dates.date2num(dateparser.parse(item)))\n",
    "        \n",
    "    mu=np.mean(t)\n",
    "    sigma=np.std(t)\n",
    "    \n",
    "    t2=[]\n",
    "    sent2=[]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Number of comments')\n",
    "    \n",
    "    val0=[]\n",
    "    val1=[]\n",
    "    \n",
    "    for i, item in enumerate(t):\n",
    "        if np.abs((item-mu)/sigma)<10:\n",
    "            t2.append(item)\n",
    "            sent2.append(sent[i])\n",
    "    \n",
    "    if(sentiment_type=='five'):\n",
    "        val2=[]\n",
    "        val3=[]\n",
    "        val4=[]\n",
    "        for i in range(0,len(sent2)-1):\n",
    "            if sent2[i]==1:\n",
    "                val0.append(t2[i])\n",
    "            if sent2[i]==2:\n",
    "                val1.append(t2[i])\n",
    "            if sent2[i]==3:\n",
    "                val2.append(t2[i])\n",
    "            if sent2[i]==4:\n",
    "                val3.append(t2[i])\n",
    "            if sent2[i]==5:\n",
    "                val4.append(t2[i])\n",
    "\n",
    "\n",
    "    \n",
    "        ax.hist([val0,val1,val2,val3,val4],log=True,color=c,label=['1=negative','2','3','4','5=positive'],bins=bins)\n",
    "        \n",
    "    if(sentiment_type=='binary'):\n",
    "        for i in range(0,len(sent2)-1):\n",
    "            if sent2[i]=='0':\n",
    "                val0.append(t2[i])\n",
    "            if sent2[i]=='1':\n",
    "                val1.append(t2[i])  \n",
    "        ax.hist([val0,val1],log=True,color=c_b,label=['0=negative','1=positive'], bins=bins)\n",
    "\n",
    "    #draw a red line at the time of the post\n",
    "    matplotlib.pyplot.axvline(x=matplotlib.dates.date2num(dateparser.parse(pt)),color='blue')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    locator = matplotlib.dates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    ax.xaxis.set_major_formatter(matplotlib.dates.AutoDateFormatter(locator))\n",
    "    fig.autofmt_xdate()\n",
    "    matplotlib.pyplot.title('Sentiment in function of the time')\n",
    "    plt.ylim(bottom=0.1)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    return\n",
    "\n",
    "# Give the correspondance between likes and the sentiment value of the comments\n",
    "#plot number of comments VS numbers of likes on the comment with the sentimal value as a parameter \n",
    "def SentimentVSCommentsLikes(fb, sentiment_type):\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    likes=fb['comment likes']\n",
    "    \n",
    "    postId=fb['post id']\n",
    "    postId=postId[1]\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "   \n",
    "    likes0=[]\n",
    "    likes1=[]\n",
    "    \n",
    "    if(sentiment_type=='five'):\n",
    "        likes2=[]\n",
    "        likes3=[]\n",
    "        likes4=[]\n",
    "        for i in range(0,len(fb)-1):\n",
    "            if fb.ix[i]['comment sentiment']==1:\n",
    "                likes0.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']==2:\n",
    "                likes1.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']==3:\n",
    "                likes2.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']==4:\n",
    "                likes3.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']==5:\n",
    "                likes4.append(likes[i])\n",
    "\n",
    "        \n",
    "        max_likes=max(likes)\n",
    "        max_likes=np.ceil(np.log2(max_likes))\n",
    "        \n",
    "        cnt0=[]\n",
    "        cnt1=[]\n",
    "        cnt2=[]\n",
    "        cnt3=[]\n",
    "        cnt4=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        cnt0.append((likes0==np.float64(0)).sum())\n",
    "        cnt1.append((likes1==np.float64(0)).sum())\n",
    "        cnt2.append((likes2==np.float64(0)).sum())\n",
    "        cnt3.append((likes3==np.float64(0)).sum())\n",
    "        cnt4.append((likes4==np.float64(0)).sum())\n",
    "        \n",
    "        \n",
    "        title.append('0')\n",
    "        \n",
    "        for i in np.arange(1,max_likes+1):\n",
    "            cnt0.append(((likes0>=np.power(2,i-1))&(likes0<np.power(2,i))).sum())\n",
    "            cnt1.append(((likes1>=np.power(2,i-1))&(likes1<np.power(2,i))).sum())\n",
    "            cnt2.append(((likes2>=np.power(2,i-1))&(likes2<np.power(2,i))).sum())\n",
    "            cnt3.append(((likes3>=np.power(2,i-1))&(likes3<np.power(2,i))).sum())\n",
    "            cnt4.append(((likes4>=np.power(2,i-1))&(likes4<np.power(2,i))).sum())\n",
    "            \n",
    "            if i==1:\n",
    "                title.append('1')\n",
    "            else:\n",
    "                title.append((np.power(2,i-1)).astype(int).astype('str')+ ' to ' + (np.power(2,i)-1).astype(int).astype('str'))\n",
    "        \n",
    "        \n",
    "        #ax.hist([likes0,likes1,likes2,likes3,likes4],log=True,color=c,label=['1=negative','2','3','4','5=positive'],bins=[-0.5,0.5,2.5,6.5,14.5,30.5,62.5,126.5,254.5,510.5,1022.5])\n",
    "       \n",
    "        delay=0.7\n",
    "        label_five=['1=negative','2','3','4','5=positive']\n",
    "        r=np.arange(max_likes+1)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt0,width=delay*0.2,color=c[0],log=True,label=label_five[0])\n",
    "        ax.bar(r+delay*0.2+(1-delay)/2,cnt1,width=delay*0.2,color=c[1],log=True,label=label_five[1])\n",
    "        ax.bar(r+delay*0.4+(1-delay)/2,cnt2,width=delay*0.2,color=c[2],log=True,label=label_five[2])\n",
    "        ax.bar(r+delay*0.6+(1-delay)/2,cnt3,width=delay*0.2,color=c[3],log=True,label=label_five[3])\n",
    "        ax.bar(r+delay*0.8+(1-delay)/2,cnt4,width=delay*0.2,color=c[4],log=True,label=label_five[4])\n",
    "        \n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "        \n",
    "        \n",
    "    if(sentiment_type=='binary'):\n",
    "        for i in range(0,len(fb)-1):\n",
    "            if fb.ix[i]['comment sentiment']=='0':\n",
    "                likes0.append(likes[i])\n",
    "            if fb.ix[i]['comment sentiment']=='1':\n",
    "                likes1.append(likes[i])\n",
    "        \n",
    "        max_likes=max(likes)\n",
    "        max_likes=np.ceil(np.log2(max_likes))\n",
    "        \n",
    "        cnt0=[]\n",
    "        cnt1=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        cnt0.append((likes0==np.float64(0)).sum())\n",
    "        cnt1.append((likes1==np.float64(0)).sum())\n",
    "                \n",
    "        title.append('0')\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_likes+1):\n",
    "            cnt0.append(((likes0>=np.power(2,i-1))&(likes0<np.power(2,i))).sum())\n",
    "            cnt1.append(((likes1>=np.power(2,i-1))&(likes1<np.power(2,i))).sum())\n",
    "            \n",
    "            if i==1:\n",
    "                title.append('1')\n",
    "            else:\n",
    "                title.append((np.power(2,i-1)).astype(int).astype('str')+ ' to ' + (np.power(2,i)-1).astype(int).astype('str'))\n",
    "\n",
    "        label_bin=['0=negative','1=positive']\n",
    "        \n",
    "        delay=0.7\n",
    "        r=np.arange(max_likes+1)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt0,width=delay*0.5,color=c_b[0],log=True,label=label_bin[0])\n",
    "        ax.bar(r+delay*0.5+(1-delay)/2,cnt1,width=delay*0.5,color=c_b[1],log=True,label=label_bin[1])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "        \n",
    "    \n",
    "    plt.xlabel('Number of likes')\n",
    "    plt.ylabel('Number of comments')\n",
    "    matplotlib.pyplot.title('Sentiment in function of the number of likes')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    plt.ylim(bottom=0.1)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# function for :  Sentiment evaluation over all comments of a post  (not in fucntion of the time)\n",
    "def ReactionToAPost(fb,sentiment_type):\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    mes=fb['comment message']\n",
    "    pm=fb['post message']\n",
    "    pm=pm[1]\n",
    "\n",
    "    \n",
    "    postId=fb['post id']\n",
    "    postId=postId[1]\n",
    "\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    plt.xlabel('Sentiment Value')\n",
    "    plt.ylabel('Number of comments')\n",
    "    \n",
    "    val0=[]\n",
    "    val1=[]\n",
    "    \n",
    "    if(sentiment_type=='five'):\n",
    "        val2=[]\n",
    "        val3=[]\n",
    "        val4=[]\n",
    "        for i in range(0,len(fb)-1):\n",
    "            if fb.ix[i]['comment sentiment']==1:\n",
    "                val0.append(1)\n",
    "            if fb.ix[i]['comment sentiment']==2:\n",
    "                val1.append(2)\n",
    "            if fb.ix[i]['comment sentiment']==3:\n",
    "                val2.append(3)\n",
    "            if fb.ix[i]['comment sentiment']==4:\n",
    "                val3.append(4)\n",
    "            if fb.ix[i]['comment sentiment']==5:\n",
    "                val4.append(5)\n",
    "            \n",
    "        \n",
    "        delay=0.7\n",
    "        \n",
    "        ax.bar(np.arange((1-delay)/2,5+(1-delay)/2,1),[len(val0),len(val1),len(val2),len(val3),len(val4)],width=delay,color=c)\n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator([0.5,1.5,2.5,3.5,4.5]))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((['1','2','3','4','5'])))\n",
    "        \n",
    "    if(sentiment_type=='binary'):\n",
    "        for i in range(0,len(fb)-1):\n",
    "            if fb.ix[i]['comment sentiment']=='0':\n",
    "                val0.append(0)\n",
    "            if fb.ix[i]['comment sentiment']=='1':\n",
    "                val1.append(1) \n",
    "        \n",
    "        delay=0.7\n",
    "        \n",
    "        ax.bar([(1-delay)/2,1+(1-delay)/2],[len(val0),len(val1)],width=delay,color=c_b)\n",
    "        ax.set_xticks=([0.5,1.5])\n",
    "        ax.set_xticklabels(('','0=negative','','1=positive'))\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    matplotlib.pyplot.title('Number of comment expressing each sentiment')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test\n",
    "SentimentVSTimefForAPost(fb_post_b,'binary',50)\n",
    "SentimentVSCommentsLikes(fb_post_b,'binary')\n",
    "ReactionToAPost(fb_post_b,'binary')\n",
    "post_sentiment=fb_post_b['post sentiment']\n",
    "post_sentiment=post_sentiment[1]\n",
    "print(\"Post sentiment: \", post_sentiment)\n",
    "\n",
    "#test\n",
    "SentimentVSTimefForAPost(fb_post,'five',50)\n",
    "SentimentVSCommentsLikes(fb_post,'five')\n",
    "ReactionToAPost(fb_post,'five')\n",
    "post_sentiment=fb_post['post sentiment']\n",
    "post_sentiment=post_sentiment[1]\n",
    "print(\"Post sentiment: \", post_sentiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for a page analysis"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Functions for  a page analysis\n",
    "\n",
    "# function for :Look the audience response to the posts in function of the time of the post. \n",
    "    \n",
    "# look at the general audience response on the page.\n",
    "\n",
    "import dateutil.parser as dateparser\n",
    "import matplotlib\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# function for :If positive post , more likes ? more comments ? more positives/negatives comments ?. (also histogramme)\n",
    "def PostsSentimentVSPostsLikes(fb,sentiment_type):\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    likes=fb['post likes']\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    \n",
    "    \n",
    "    max_likes=max(likes)\n",
    "    slot_width=np.ceil(max_likes/10)\n",
    "\n",
    "    if (np.modf(np.log10(slot_width))[0]>=0.2) and (np.modf(np.log10(slot_width))[0]<0.5):\n",
    "        slot_width=2*(np.power(10,np.floor(np.log10(slot_width))))\n",
    "    else:\n",
    "        if (np.modf(np.log10(slot_width))[0]>=0.5) and (np.modf(np.log10(slot_width))[0]<0.8):\n",
    "            slot_width=5*(np.power(10,np.trunc(np.log10(slot_width))))\n",
    "        else:\n",
    "            slot_width=(np.power(10,np.round(np.log10(slot_width))))\n",
    "\n",
    "    max_likes=np.ceil(max_likes/slot_width)\n",
    "    \n",
    "\n",
    "    likes1=[]\n",
    "\n",
    "    if(sentiment_type=='five'):\n",
    "        likes2=[]\n",
    "        likes3=[]\n",
    "        likes4=[]\n",
    "        likes5=[]\n",
    "        i=0\n",
    "        x = True\n",
    "        newPostId=''\n",
    "        while(x) :\n",
    "            lastPostId=newPostId\n",
    "            newPostId=fb.ix[i]['post id']\n",
    "                \n",
    "            if fb.ix[i]['post sentiment']==1:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes1.append(likes[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==2:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes2.append(likes[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==3:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes3.append(likes[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==4:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes4.append(likes[i])\n",
    "                i = i + 1 \n",
    "            if fb.ix[i]['post sentiment']==5:\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes5.append(likes[i])\n",
    "                i = i + 1\n",
    "            if i == len(fb)-1:\n",
    "                x=False\n",
    "                \n",
    "        cnt1=[]\n",
    "        cnt2=[]\n",
    "        cnt3=[]\n",
    "        cnt4=[]\n",
    "        cnt5=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_likes+1):\n",
    "            cnt1.append(((likes1>=slot_width*(i-1))&(likes1<slot_width*i)).sum())\n",
    "            cnt2.append(((likes2>=slot_width*(i-1))&(likes2<slot_width*i)).sum())\n",
    "            cnt3.append(((likes3>=slot_width*(i-1))&(likes3<slot_width*i)).sum())\n",
    "            cnt4.append(((likes4>=slot_width*(i-1))&(likes4<slot_width*i)).sum())\n",
    "            cnt5.append(((likes5>=slot_width*(i-1))&(likes5<slot_width*i)).sum())\n",
    "            title.append((slot_width*(i-1)).astype(int).astype('str')+ ' to ' + (slot_width*i-1).astype(int).astype('str'))\n",
    "        \n",
    "        delay=0.7\n",
    "        label_five=['1=negative','2','3','4','5=positive']\n",
    "        r=np.arange(max_likes)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt1,width=delay*0.5,color=c[0],label=label_five[0])\n",
    "        ax.bar(r+delay*0.2+(1-delay)/2,cnt2,width=delay*0.2,color=c[1],label=label_five[1])\n",
    "        ax.bar(r+delay*0.4+(1-delay)/2,cnt3,width=delay*0.2,color=c[2],label=label_five[2])\n",
    "        ax.bar(r+delay*0.6+(1-delay)/2,cnt4,width=delay*0.2,color=c[3],label=label_five[3])\n",
    "        ax.bar(r+delay*0.8+(1-delay)/2,cnt5,width=delay*0.2,color=c[4],label=label_five[4])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "        \n",
    "        #ax.hist([likes1,likes2,likes3,likes4,likes5],color=c,label=['1=negative','2','3','4','5=positive'])\n",
    "   \n",
    "    if(sentiment_type=='binary'):\n",
    "        likes0=[]\n",
    "        x=True\n",
    "        i=0\n",
    "        newPostId=''\n",
    "        while x:\n",
    "            lastPostId=newPostId\n",
    "            newPostId=fb.ix[i]['post id']\n",
    "            if fb.ix[i]['post sentiment']=='0':\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes0.append(likes[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']=='1':\n",
    "                if(lastPostId != newPostId):\n",
    "                    likes1.append(likes[i])\n",
    "                i = i + 1\n",
    "            if i == len(fb)-1:\n",
    "                x=False\n",
    "               \n",
    "        cnt0=[]\n",
    "        cnt1=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_likes+1):\n",
    "            cnt0.append(((likes0>=slot_width*(i-1))&(likes0<slot_width*i)).sum())\n",
    "            cnt1.append(((likes1>=slot_width*(i-1))&(likes1<slot_width*i)).sum())\n",
    "            title.append((slot_width*(i-1)).astype(int).astype('str')+ ' to ' + (slot_width*i-1).astype(int).astype('str'))\n",
    "\n",
    "        label_bin=['0=negative','1=positive']\n",
    "                \n",
    "        #ax.hist([likes0,likes1],color=c_b,label=['0=negative','1=positive'])\n",
    "        \n",
    "        delay=0.7\n",
    "        \n",
    "        r=np.arange(max_likes)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt0,width=delay*0.5,color=c_b[0],label=label_bin[0])\n",
    "        ax.bar(r+delay*0.5+(1-delay)/2,cnt1,width=delay*0.5,color=c_b[1],label=label_bin[1])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "\n",
    "\n",
    "    \n",
    "    plt.xlabel('Number of likes')\n",
    "    plt.ylabel('Number of posts')\n",
    "    matplotlib.pyplot.title('Posts sentiment VS Posts likes')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def PostsSentimentsVSCommentsCount(fb,sentiment_type):\n",
    "    c=['red','orange','yellow','greenyellow','green']\n",
    "    c_b=['red','green']\n",
    "    count=fb['nb of comments']\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1,figsize=(20,10))\n",
    "    \n",
    "    \n",
    "    max_count=max(count)\n",
    "    slot_width=np.ceil(max_count/10)\n",
    "\n",
    "    if (np.modf(np.log10(slot_width))[0]>=0.2) and (np.modf(np.log10(slot_width))[0]<0.5):\n",
    "        slot_width=2*(np.power(10,np.floor(np.log10(slot_width))))\n",
    "    else:\n",
    "        if (np.modf(np.log10(slot_width))[0]>=0.5) and (np.modf(np.log10(slot_width))[0]<0.8):\n",
    "            slot_width=5*(np.power(10,np.trunc(np.log10(slot_width))))\n",
    "        else:\n",
    "            slot_width=(np.power(10,np.round(np.log10(slot_width))))\n",
    "\n",
    "    max_count=np.ceil(max_count/slot_width)\n",
    "    \n",
    "   \n",
    "    count1=[]\n",
    "\n",
    "    if(sentiment_type=='five'):\n",
    "        count2=[]\n",
    "        count3=[]\n",
    "        count4=[]\n",
    "        count5=[]\n",
    "        \n",
    "        i=0\n",
    "        x = True\n",
    "        newPostId=''\n",
    "        while(x) :\n",
    "            lastPostId=newPostId\n",
    "            newPostId=fb.ix[i]['post id']\n",
    "                \n",
    "            if fb.ix[i]['post sentiment']==1:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count1.append(count[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==2:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count2.append(count[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==3:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count3.append(count[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']==4:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count4.append(count[i])\n",
    "                i = i + 1 \n",
    "            if fb.ix[i]['post sentiment']==5:\n",
    "                if(lastPostId != newPostId):\n",
    "                    count5.append(count[i])\n",
    "                i = i + 1\n",
    "            if i == len(fb)-1:\n",
    "                x=False\n",
    "                \n",
    "        cnt1=[]\n",
    "        cnt2=[]\n",
    "        cnt3=[]\n",
    "        cnt4=[]\n",
    "        cnt5=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_count+1):\n",
    "            cnt1.append(((count1>=slot_width*(i-1))&(count1<slot_width*i)).sum())\n",
    "            cnt2.append(((count2>=slot_width*(i-1))&(count2<slot_width*i)).sum())\n",
    "            cnt3.append(((count3>=slot_width*(i-1))&(count3<slot_width*i)).sum())\n",
    "            cnt4.append(((count4>=slot_width*(i-1))&(count4<slot_width*i)).sum())\n",
    "            cnt5.append(((count5>=slot_width*(i-1))&(count5<slot_width*i)).sum())\n",
    "            title.append((slot_width*(i-1)).astype(int).astype('str')+ ' to ' + (slot_width*i-1).astype(int).astype('str'))\n",
    "        \n",
    "        delay=0.7\n",
    "        label_five=['1=negative','2','3','4','5=positive']\n",
    "        \n",
    "        r=np.arange(max_count)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt1,width=delay*0.5,color=c[0],label=label_five[0])\n",
    "        ax.bar(r+delay*0.2+(1-delay)/2,cnt2,width=delay*0.2,color=c[1],label=label_five[1])\n",
    "        ax.bar(r+delay*0.4+(1-delay)/2,cnt3,width=delay*0.2,color=c[2],label=label_five[2])\n",
    "        ax.bar(r+delay*0.6+(1-delay)/2,cnt4,width=delay*0.2,color=c[3],label=label_five[3])\n",
    "        ax.bar(r+delay*0.8+(1-delay)/2,cnt5,width=delay*0.2,color=c[4],label=label_five[4])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "                \n",
    "        \n",
    "        #ax.hist([count1,count2,count3,count4,count5],color=c,label=['1=negative','2','3','4','5=positive'])\n",
    "   \n",
    "    if(sentiment_type=='binary'):\n",
    "        count0=[]\n",
    "        x=True\n",
    "        i=0\n",
    "        newPostId=''\n",
    "        while x:\n",
    "            lastPostId=newPostId\n",
    "            newPostId=fb.ix[i]['post id']\n",
    "            if fb.ix[i]['post sentiment']=='0':\n",
    "                if(lastPostId != newPostId):\n",
    "                    count0.append(count[i])\n",
    "                i = i + 1\n",
    "            if fb.ix[i]['post sentiment']=='1':\n",
    "                if(lastPostId != newPostId):\n",
    "                    count1.append(count[i])\n",
    "                i = i + 1\n",
    "            if i == len(fb)-1:\n",
    "                x=False\n",
    "                \n",
    "        cnt0=[]\n",
    "        cnt1=[]\n",
    "        \n",
    "        title=[]\n",
    "        \n",
    "        \n",
    "        for i in np.arange(1,max_count+1):\n",
    "            cnt0.append(((count0>=slot_width*(i-1))&(count0<slot_width*i)).sum())\n",
    "            cnt1.append(((count1>=slot_width*(i-1))&(count1<slot_width*i)).sum())\n",
    "            title.append((slot_width*(i-1)).astype(int).astype('str')+ ' to ' + (slot_width*i-1).astype(int).astype('str'))\n",
    "\n",
    "        label_bin=['0=negative','1=positive']\n",
    "                \n",
    "        #ax.hist([likes0,likes1],color=c_b,label=['0=negative','1=positive'])\n",
    "        \n",
    "        delay=0.7\n",
    "        \n",
    "        r=np.arange(max_count)\n",
    "        \n",
    "        ax.bar(r+(1-delay)/2,cnt0,width=delay*0.5,color=c_b[0],label=label_bin[0])\n",
    "        ax.bar(r+delay*0.5+(1-delay)/2,cnt1,width=delay*0.5,color=c_b[1],label=label_bin[1])\n",
    "        \n",
    "        ax.xaxis.set_major_locator(ticker.FixedLocator(r+0.5))\n",
    "        ax.xaxis.set_major_formatter(ticker.FixedFormatter((title)))\n",
    "                \n",
    "        #ax.hist([count0,count1],color=c_b,label=['0=negative','1=positive'])\n",
    "\n",
    "\n",
    "    \n",
    "    plt.xlabel('Number of comments')\n",
    "    plt.ylabel('Number of posts')\n",
    "    matplotlib.pyplot.title('Post sentiment VS Numbers of comments ')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "    plt.show()\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test Binary\n",
    "PostsSentimentVSPostsLikes(fb_page_b,'binary')\n",
    "PostsSentimentsVSCommentsCount(fb_page_b,'binary')\n",
    "\n",
    "#test AMAZON\n",
    "PostsSentimentVSPostsLikes(fb_page,'five')\n",
    "PostsSentimentsVSCommentsCount(fb_page,'five')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": null,
>>>>>>> master
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllGraphsForPost(filename,n):\n",
    "\n",
    "    #Binary\n",
    "    fb_post_b =getTable(filename)\n",
    "    input_comment=getInputComment(fb_post_b,'SAT')\n",
    "    input_post=getInputPost(fb_post_b,'SAT')\n",
    "    getAndAddPrediction(input_comment, input_post,fb_post_b,'PAClassifierOnSat.pkl')\n",
    "\n",
    "\n",
    "    fb_post =getTable(filename)\n",
    "    input_comment=getInputComment(fb_post,'Amazon')\n",
    "    input_post=getInputPost(fb_post,'Amazon')\n",
    "    getAndAddPrediction(input_comment, input_post,fb_post,'PAClassifierOnAmazon.pkl')\n",
    "\n",
    "    SentimentVSTimefForAPost(fb_post_b,'binary',n)\n",
    "    SentimentVSCommentsLikes(fb_post_b,'binary')\n",
    "    ReactionToAPost(fb_post_b,'binary')\n",
    "    post_sentiment=fb_post_b['post sentiment']\n",
    "    post_sentiment=post_sentiment[1]\n",
    "    print(\"Post sentiment: \", post_sentiment)\n",
    "\n",
    "\n",
    "    SentimentVSTimefForAPost(fb_post,'five',n)\n",
    "    SentimentVSCommentsLikes(fb_post,'five')\n",
    "    ReactionToAPost(fb_post,'five')\n",
    "    post_sentiment=fb_post['post sentiment']\n",
    "    post_sentiment=post_sentiment[1]\n",
    "    print(\"Post sentiment: \", post_sentiment)\n",
    "    return\n",
    "\n",
    "def getAllGraphsForPage(filename,n):\n",
    "    \n",
    "    #BINARY\n",
    "\n",
    "    fb_page_b =getTable(filename)\n",
    "    input_comment=getInputComment(fb_page_b,'SAT')\n",
    "    input_post=getInputPost(fb_page_b,'SAT')\n",
    "    getAndAddPrediction(input_comment, input_post,fb_page_b,'PAClassifierOnSat.pkl')\n",
    "    PostsSentimentVSPostsLikes(fb_page_b,'binary')\n",
    "    PostsSentimentsVSCommentsCount(fb_page_b,'binary')\n",
    "\n",
    "    #AMAZON\n",
    "\n",
    "    fb_page =getTable(filename)\n",
    "    input_comment=getInputComment(fb_page,'Amazon')\n",
    "    input_post=getInputPost(fb_page,'Amazon')\n",
    "    getAndAddPrediction(input_comment, input_post,fb_page,'PAClassifierOnAmazon.pkl')\n",
    "    PostsSentimentVSPostsLikes(fb_page,'five')\n",
    "    PostsSentimentsVSCommentsCount(fb_page,'five')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on real data\n",
    "\n",
    "## Analysis  on posts\n",
    "\n",
    "### Theme : Terrorism\n",
    "#### The Orlando shooting\n",
    "\n",
    "CNN article about orlando shooting's victims suing twitter and facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#CNN article about orlando shooting's victims suing twitter and facebook\n",
    "\n",
    "#page='cnninternational'\n",
    "#postId= '10154812470324641'\n",
    "#fb= postReactions(postId,1,'CNN_OS1')\n",
    "filename='facebookCNN_OS1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN article about John McCain saying that Obama is responsible for the Orlando shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154219265779641'\n",
    "#fb= postReactions(postId,1,'CNN_OS2')\n",
    "filename='facebookCNN_OS2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN articles about gun sales after Orlando shooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154210843679641'\n",
    "#fb= postReactions(postId,1,'CNN_OS3')\n",
    "filename='facebookCNN_OS3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme : Aborption\n",
    "CNN articles about Lena Dunham's aborption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154817313449641'\n",
    "#fb= postReactions(postId,1,'CNN_Ab1')\n",
    "filename='facebookCNN_Ab1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN article about Texas fetus burial obliagtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154738564069641'\n",
    "#fb= postReactions(postId,1,'CNN_Ab2')\n",
    "filename='facebookCNN_Ab2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN article about pope abortion forgivness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10154708071624641'\n",
    "#fb= postReactions(postId,1,'CNN_Ab3')\n",
    "filename='facebookCNN_Ab3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme : Political\n",
    "#### Brexit\n",
    "\n",
    "David Cameron before brexit speaking about "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page = 'DavidCameronOfficial'\n",
    "#postId= '1216426805048302'\n",
    "#fb= postReactions(postId,1,'DC1')\n",
    "filename='facebookDC1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David Cameron 'thank you' note to the voters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '1218229621534687'\n",
    "#fb= postReactions(postId,0,'DC2')\n",
    "filename='facebookDC2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### US presidential election"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hillary Clinton asking for votes (before election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page = 'hillaryclinton'\n",
    "#postId= '1322830897773436'\n",
    "#fb= postReactions(postId,1,'HC3')\n",
    "filename='facebookHC3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hillary Clinton defeat speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '1324421317614394'\n",
    "#fb= postReactions(postId,0,'HC2')\n",
    "filename='facebookHC2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Trump about winning more votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page = 'DonaldTrump'\n",
    "#postId= '10158135498775725'\n",
    "#fb= postReactions(postId,0,'DT5')\n",
    "filename='facebookDT5.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Trump speaks about the defeat of the democrats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page = 'DonaldTrump'\n",
    "#postId= '10158423167515725'\n",
    "#fb= postReactions(postId,0,'DT3')\n",
    "filename='facebookDT3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donald Trump\n",
    "Donald Trump reaction to Meryl Streep speach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = 'DonaldTrump'\n",
    "postId= '10158439095420725'\n",
    "fb= postReactions(postId,0,'DT1')\n",
    "filename='facebookDT1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Trump speaks about a good relationship with Russia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10158430269170725'\n",
    "#fb= postReactions(postId,0,'DT2')\n",
    "filename='facebookDT2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donald Trump speaks about Obamacare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId= '10158417912085725'\n",
    "#fb= postReactions(postId,0,'DT4')\n",
    "filename='facebookDT4.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme : Environmental\n",
    "#### Dakota Pipeline\n",
    "\n",
    "AJ+ post on Native Americans fighting against the Dakota Access pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page='ajplusenglish'\n",
    "#postId='843345759140266'\n",
    "#fb= postReactions(postId,1,'ENV1')\n",
    "filename='facebookENV1.sqlite'\n",
    "getAllGraphsForPost(filename,50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AJ+ post on celebrations after the U.S. Army Corp of Engineers refused to grant \n",
    "an easement allowing the Dakota Access Pipeline to go under Lake Oahe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId='852012618273580'\n",
    "#fb= postReactions(postId,0,'ENV2')\n",
    "filename='facebookENV2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Donald Trump and environmental issues\n",
    "AJ+ post on the hackers that are preserving environmental data before Trump takes office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId='864548553686653'\n",
    "#fb= postReactions(postId,1,'ENV3')\n",
    "filename='facebookENV3.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AJ+ post on Donald Trump being picked for the Environmental Protection Agency not believing in man-made climate change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId='855199717954870'\n",
    "#fb= postReactions(postId,1,'ENV4')\n",
    "filename='facebookENV4.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theme : Homosexuality\n",
    "\n",
    "CNN post on Pope saying Christians should apologize to gay people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#page='cnninternational'\n",
    "#postId='10154249259664641'\n",
    "#fb= postReactions(postId,1,'HM1')\n",
    "filename='facebookHM1.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN post on  the fact that more than half of British Muslims think homosexuality should be illegal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#postId='10154056181289641'\n",
    "#fb= postReactions(postId,1,'HM2')\n",
    "#display(fb[:])\n",
    "filename='facebookHM2.sqlite'\n",
    "getAllGraphsForPost(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis  on pages\n",
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>post message</th>\n",
       "      <th>post id</th>\n",
       "      <th>post time</th>\n",
       "      <th>post likes</th>\n",
       "      <th>nb of comments</th>\n",
       "      <th>comment id</th>\n",
       "      <th>comment message</th>\n",
       "      <th>comment time</th>\n",
       "      <th>comment likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906300009641</td>\n",
       "      <td>Sorry for yr loss I hope u all charish every m...</td>\n",
       "      <td>2017-01-15 13:44:40.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906288944641</td>\n",
       "      <td>So sorry for your loss. As a mother of twins, ...</td>\n",
       "      <td>2017-01-15 13:38:48.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906243599641</td>\n",
       "      <td>This breaks my heart, 😧😦</td>\n",
       "      <td>2017-01-15 13:18:41.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906277449641</td>\n",
       "      <td>Sorry for your loss!!</td>\n",
       "      <td>2017-01-15 13:34:48.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906211039641</td>\n",
       "      <td>It breaks my heart 😢</td>\n",
       "      <td>2017-01-15 13:06:35.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906269989641</td>\n",
       "      <td>My heart is broken!</td>\n",
       "      <td>2017-01-15 13:31:46.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906256844641</td>\n",
       "      <td>It breaks my heart ❤️</td>\n",
       "      <td>2017-01-15 13:24:53.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906329179641</td>\n",
       "      <td>cute baby..</td>\n",
       "      <td>2017-01-15 14:00:51.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906320804641</td>\n",
       "      <td>Beautiful memories. Terribly saddening.</td>\n",
       "      <td>2017-01-15 13:56:43.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906302564641</td>\n",
       "      <td>So sorry, Blessing</td>\n",
       "      <td>2017-01-15 13:46:00.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906286544641</td>\n",
       "      <td></td>\n",
       "      <td>2017-01-15 13:37:09.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906293169641</td>\n",
       "      <td>Fake News</td>\n",
       "      <td>2017-01-15 13:41:11.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906263184641</td>\n",
       "      <td></td>\n",
       "      <td>2017-01-15 13:28:21.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906217244641</td>\n",
       "      <td>Beautiful in every way</td>\n",
       "      <td>2017-01-15 13:08:43.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906261529641</td>\n",
       "      <td>Beautiful baby in heaven !!! my grandbaby is i...</td>\n",
       "      <td>2017-01-15 13:27:22.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906297204641</td>\n",
       "      <td>Bellos</td>\n",
       "      <td>2017-01-15 13:43:36.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906206364641</td>\n",
       "      <td>so cute</td>\n",
       "      <td>2017-01-15 13:03:39.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906223394641</td>\n",
       "      <td>sleep now</td>\n",
       "      <td>2017-01-15 13:11:22.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906337619641</td>\n",
       "      <td>i feel dearly for the parents of the child</td>\n",
       "      <td>2017-01-15 14:03:23.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906335324641</td>\n",
       "      <td>such a sad moment</td>\n",
       "      <td>2017-01-15 14:02:43.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906302854641</td>\n",
       "      <td>Imat Belarde</td>\n",
       "      <td>2017-01-15 13:46:14.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906302384641</td>\n",
       "      <td>Distracted daily basics,,,,,https://m.facebook...</td>\n",
       "      <td>2017-01-15 13:45:43.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>A newborn twin wasn't long for this world, but...</td>\n",
       "      <td>18793419640_10154906198799641</td>\n",
       "      <td>2017-01-15 13:01:40.000000</td>\n",
       "      <td>286</td>\n",
       "      <td>25</td>\n",
       "      <td>10154906198799641_10154906271214641</td>\n",
       "      <td>Eswaran Balasubramanian</td>\n",
       "      <td>2017-01-15 13:32:32.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>The Ringling Bros. circus is closing down afte...</td>\n",
       "      <td>18793419640_10154905871919641</td>\n",
       "      <td>2017-01-15 10:17:37.000000</td>\n",
       "      <td>1353</td>\n",
       "      <td>115</td>\n",
       "      <td>10154905871919641_10154905881929641</td>\n",
       "      <td>Good! Let's get those animals back in the wild...</td>\n",
       "      <td>2017-01-15 10:20:39.000000</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>The Ringling Bros. circus is closing down afte...</td>\n",
       "      <td>18793419640_10154905871919641</td>\n",
       "      <td>2017-01-15 10:17:37.000000</td>\n",
       "      <td>1353</td>\n",
       "      <td>115</td>\n",
       "      <td>10154905871919641_10154905993879641</td>\n",
       "      <td>Circuses, zoos, aquatic parks, and anything el...</td>\n",
       "      <td>2017-01-15 11:02:45.000000</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>The Ringling Bros. circus is closing down afte...</td>\n",
       "      <td>18793419640_10154905871919641</td>\n",
       "      <td>2017-01-15 10:17:37.000000</td>\n",
       "      <td>1353</td>\n",
       "      <td>115</td>\n",
       "      <td>10154905871919641_10154906165474641</td>\n",
       "      <td>Sadly! No support from audience who love to wa...</td>\n",
       "      <td>2017-01-15 12:41:30.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>The Ringling Bros. circus is closing down afte...</td>\n",
       "      <td>18793419640_10154905871919641</td>\n",
       "      <td>2017-01-15 10:17:37.000000</td>\n",
       "      <td>1353</td>\n",
       "      <td>115</td>\n",
       "      <td>10154905871919641_10154906116014641</td>\n",
       "      <td>While it is a victory for animal activists. I ...</td>\n",
       "      <td>2017-01-15 12:13:14.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>The Ringling Bros. circus is closing down afte...</td>\n",
       "      <td>18793419640_10154905871919641</td>\n",
       "      <td>2017-01-15 10:17:37.000000</td>\n",
       "      <td>1353</td>\n",
       "      <td>115</td>\n",
       "      <td>10154905871919641_10154905928959641</td>\n",
       "      <td>While the animals retiring is good for them, I...</td>\n",
       "      <td>2017-01-15 10:37:30.000000</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>The Ringling Bros. circus is closing down afte...</td>\n",
       "      <td>18793419640_10154905871919641</td>\n",
       "      <td>2017-01-15 10:17:37.000000</td>\n",
       "      <td>1353</td>\n",
       "      <td>115</td>\n",
       "      <td>10154905871919641_10154906171519641</td>\n",
       "      <td>Thank you God for finally making us realize th...</td>\n",
       "      <td>2017-01-15 12:45:25.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>The Ringling Bros. circus is closing down afte...</td>\n",
       "      <td>18793419640_10154905871919641</td>\n",
       "      <td>2017-01-15 10:17:37.000000</td>\n",
       "      <td>1353</td>\n",
       "      <td>115</td>\n",
       "      <td>10154905871919641_10154906116099641</td>\n",
       "      <td>Circuses were ok in the early 1900's for famil...</td>\n",
       "      <td>2017-01-15 12:13:16.000000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>3021</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154905126124641</td>\n",
       "      <td>Michael Carney</td>\n",
       "      <td>2017-01-15 03:20:27.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>3022</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154904915724641</td>\n",
       "      <td>Jean Paul Langenstein</td>\n",
       "      <td>2017-01-15 01:48:31.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3023</th>\n",
       "      <td>3023</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902514564641</td>\n",
       "      <td>Nidal Eses</td>\n",
       "      <td>2017-01-14 08:33:17.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>3024</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154904101599641</td>\n",
       "      <td>Amy Therriault</td>\n",
       "      <td>2017-01-14 19:55:39.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3025</th>\n",
       "      <td>3025</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903286809641</td>\n",
       "      <td>Nick</td>\n",
       "      <td>2017-01-14 14:34:25.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3026</th>\n",
       "      <td>3026</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902496369641</td>\n",
       "      <td>Ann Marie Kennedy</td>\n",
       "      <td>2017-01-14 08:24:02.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>3027</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903423639641</td>\n",
       "      <td>Gregory James Gotthardt</td>\n",
       "      <td>2017-01-14 15:26:18.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>3028</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902396014641</td>\n",
       "      <td>Mike Penna</td>\n",
       "      <td>2017-01-14 07:34:34.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3029</th>\n",
       "      <td>3029</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154905817669641</td>\n",
       "      <td>Karma Wangyal Grg</td>\n",
       "      <td>2017-01-15 09:58:11.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3030</th>\n",
       "      <td>3030</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154904434374641</td>\n",
       "      <td>Brandon Bonilla</td>\n",
       "      <td>2017-01-14 22:00:44.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3031</th>\n",
       "      <td>3031</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902421239641</td>\n",
       "      <td>Adineri Marquez</td>\n",
       "      <td>2017-01-14 07:43:59.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>3032</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903541809641</td>\n",
       "      <td>Aaron Zeier</td>\n",
       "      <td>2017-01-14 16:13:18.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3033</th>\n",
       "      <td>3033</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154906300784641</td>\n",
       "      <td>Sebastian Castro</td>\n",
       "      <td>2017-01-15 13:45:11.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>3034</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154905583869641</td>\n",
       "      <td>Matthew Bell</td>\n",
       "      <td>2017-01-15 07:37:29.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>3035</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903433019641</td>\n",
       "      <td>Gary Lee</td>\n",
       "      <td>2017-01-14 15:30:34.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>3036</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902472169641</td>\n",
       "      <td>Gui Lherme</td>\n",
       "      <td>2017-01-14 08:09:45.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>3037</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902757359641</td>\n",
       "      <td>Astrid Francoys</td>\n",
       "      <td>2017-01-14 10:30:30.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>3038</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902521774641</td>\n",
       "      <td>Bailey Anabelle Lesslie</td>\n",
       "      <td>2017-01-14 08:38:35.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>3039</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903584594641</td>\n",
       "      <td>Guillaume Landry</td>\n",
       "      <td>2017-01-14 16:32:16.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>3040</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903474139641</td>\n",
       "      <td>Dominique Vallée</td>\n",
       "      <td>2017-01-14 15:49:06.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>3041</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902434639641</td>\n",
       "      <td>Brittany Brandi Best</td>\n",
       "      <td>2017-01-14 07:50:05.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>3042</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903987854641</td>\n",
       "      <td>Sara Nilsson</td>\n",
       "      <td>2017-01-14 19:08:40.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>3043</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154904205654641</td>\n",
       "      <td>Kent Larsen</td>\n",
       "      <td>2017-01-14 20:34:04.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>3044</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902825839641</td>\n",
       "      <td>Jamie Buttigieg</td>\n",
       "      <td>2017-01-14 10:56:49.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>3045</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902533434641</td>\n",
       "      <td>Silas Ayers</td>\n",
       "      <td>2017-01-14 08:43:56.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>3046</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902985364641</td>\n",
       "      <td>LA FLAMME DE WILLY MUFFLER EST ACTIVÉE POUR LE...</td>\n",
       "      <td>2017-01-14 12:12:00.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>3047</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154905705679641</td>\n",
       "      <td>Krystle Kaye Lemly</td>\n",
       "      <td>2017-01-15 09:00:15.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>3048</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154902432389641</td>\n",
       "      <td>FLAT EARTH  -- LOOK AT THE RAINBOW!\\nhttps://w...</td>\n",
       "      <td>2017-01-14 07:48:59.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>3049</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903473514641</td>\n",
       "      <td>Jordan Moisan</td>\n",
       "      <td>2017-01-14 15:48:50.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>3050</td>\n",
       "      <td>Volkswagen recently unveiled the ID Buzz, an a...</td>\n",
       "      <td>18793419640_10154902391029641</td>\n",
       "      <td>2017-01-14 07:31:46.000000</td>\n",
       "      <td>1566</td>\n",
       "      <td>138</td>\n",
       "      <td>10154902391029641_10154903717249641</td>\n",
       "      <td>Tobi Naumann</td>\n",
       "      <td>2017-01-14 17:32:40.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3051 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                       post message  \\\n",
       "0         0  A newborn twin wasn't long for this world, but...   \n",
       "1         1  A newborn twin wasn't long for this world, but...   \n",
       "2         2  A newborn twin wasn't long for this world, but...   \n",
       "3         3  A newborn twin wasn't long for this world, but...   \n",
       "4         4  A newborn twin wasn't long for this world, but...   \n",
       "5         5  A newborn twin wasn't long for this world, but...   \n",
       "6         6  A newborn twin wasn't long for this world, but...   \n",
       "7         7  A newborn twin wasn't long for this world, but...   \n",
       "8         8  A newborn twin wasn't long for this world, but...   \n",
       "9         9  A newborn twin wasn't long for this world, but...   \n",
       "10       10  A newborn twin wasn't long for this world, but...   \n",
       "11       11  A newborn twin wasn't long for this world, but...   \n",
       "12       12  A newborn twin wasn't long for this world, but...   \n",
       "13       13  A newborn twin wasn't long for this world, but...   \n",
       "14       14  A newborn twin wasn't long for this world, but...   \n",
       "15       15  A newborn twin wasn't long for this world, but...   \n",
       "16       16  A newborn twin wasn't long for this world, but...   \n",
       "17       17  A newborn twin wasn't long for this world, but...   \n",
       "18       18  A newborn twin wasn't long for this world, but...   \n",
       "19       19  A newborn twin wasn't long for this world, but...   \n",
       "20       20  A newborn twin wasn't long for this world, but...   \n",
       "21       21  A newborn twin wasn't long for this world, but...   \n",
       "22       22  A newborn twin wasn't long for this world, but...   \n",
       "23       23  The Ringling Bros. circus is closing down afte...   \n",
       "24       24  The Ringling Bros. circus is closing down afte...   \n",
       "25       25  The Ringling Bros. circus is closing down afte...   \n",
       "26       26  The Ringling Bros. circus is closing down afte...   \n",
       "27       27  The Ringling Bros. circus is closing down afte...   \n",
       "28       28  The Ringling Bros. circus is closing down afte...   \n",
       "29       29  The Ringling Bros. circus is closing down afte...   \n",
       "...     ...                                                ...   \n",
       "3021   3021  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3022   3022  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3023   3023  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3024   3024  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3025   3025  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3026   3026  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3027   3027  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3028   3028  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3029   3029  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3030   3030  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3031   3031  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3032   3032  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3033   3033  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3034   3034  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3035   3035  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3036   3036  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3037   3037  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3038   3038  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3039   3039  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3040   3040  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3041   3041  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3042   3042  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3043   3043  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3044   3044  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3045   3045  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3046   3046  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3047   3047  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3048   3048  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3049   3049  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "3050   3050  Volkswagen recently unveiled the ID Buzz, an a...   \n",
       "\n",
       "                            post id                   post time  post likes  \\\n",
       "0     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "1     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "2     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "3     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "4     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "5     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "6     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "7     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "8     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "9     18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "10    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "11    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "12    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "13    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "14    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "15    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "16    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "17    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "18    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "19    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "20    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "21    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "22    18793419640_10154906198799641  2017-01-15 13:01:40.000000         286   \n",
       "23    18793419640_10154905871919641  2017-01-15 10:17:37.000000        1353   \n",
       "24    18793419640_10154905871919641  2017-01-15 10:17:37.000000        1353   \n",
       "25    18793419640_10154905871919641  2017-01-15 10:17:37.000000        1353   \n",
       "26    18793419640_10154905871919641  2017-01-15 10:17:37.000000        1353   \n",
       "27    18793419640_10154905871919641  2017-01-15 10:17:37.000000        1353   \n",
       "28    18793419640_10154905871919641  2017-01-15 10:17:37.000000        1353   \n",
       "29    18793419640_10154905871919641  2017-01-15 10:17:37.000000        1353   \n",
       "...                             ...                         ...         ...   \n",
       "3021  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3022  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3023  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3024  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3025  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3026  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3027  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3028  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3029  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3030  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3031  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3032  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3033  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3034  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3035  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3036  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3037  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3038  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3039  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3040  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3041  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3042  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3043  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3044  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3045  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3046  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3047  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3048  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3049  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "3050  18793419640_10154902391029641  2017-01-14 07:31:46.000000        1566   \n",
       "\n",
       "      nb of comments                           comment id  \\\n",
       "0                 25  10154906198799641_10154906300009641   \n",
       "1                 25  10154906198799641_10154906288944641   \n",
       "2                 25  10154906198799641_10154906243599641   \n",
       "3                 25  10154906198799641_10154906277449641   \n",
       "4                 25  10154906198799641_10154906211039641   \n",
       "5                 25  10154906198799641_10154906269989641   \n",
       "6                 25  10154906198799641_10154906256844641   \n",
       "7                 25  10154906198799641_10154906329179641   \n",
       "8                 25  10154906198799641_10154906320804641   \n",
       "9                 25  10154906198799641_10154906302564641   \n",
       "10                25  10154906198799641_10154906286544641   \n",
       "11                25  10154906198799641_10154906293169641   \n",
       "12                25  10154906198799641_10154906263184641   \n",
       "13                25  10154906198799641_10154906217244641   \n",
       "14                25  10154906198799641_10154906261529641   \n",
       "15                25  10154906198799641_10154906297204641   \n",
       "16                25  10154906198799641_10154906206364641   \n",
       "17                25  10154906198799641_10154906223394641   \n",
       "18                25  10154906198799641_10154906337619641   \n",
       "19                25  10154906198799641_10154906335324641   \n",
       "20                25  10154906198799641_10154906302854641   \n",
       "21                25  10154906198799641_10154906302384641   \n",
       "22                25  10154906198799641_10154906271214641   \n",
       "23               115  10154905871919641_10154905881929641   \n",
       "24               115  10154905871919641_10154905993879641   \n",
       "25               115  10154905871919641_10154906165474641   \n",
       "26               115  10154905871919641_10154906116014641   \n",
       "27               115  10154905871919641_10154905928959641   \n",
       "28               115  10154905871919641_10154906171519641   \n",
       "29               115  10154905871919641_10154906116099641   \n",
       "...              ...                                  ...   \n",
       "3021             138  10154902391029641_10154905126124641   \n",
       "3022             138  10154902391029641_10154904915724641   \n",
       "3023             138  10154902391029641_10154902514564641   \n",
       "3024             138  10154902391029641_10154904101599641   \n",
       "3025             138  10154902391029641_10154903286809641   \n",
       "3026             138  10154902391029641_10154902496369641   \n",
       "3027             138  10154902391029641_10154903423639641   \n",
       "3028             138  10154902391029641_10154902396014641   \n",
       "3029             138  10154902391029641_10154905817669641   \n",
       "3030             138  10154902391029641_10154904434374641   \n",
       "3031             138  10154902391029641_10154902421239641   \n",
       "3032             138  10154902391029641_10154903541809641   \n",
       "3033             138  10154902391029641_10154906300784641   \n",
       "3034             138  10154902391029641_10154905583869641   \n",
       "3035             138  10154902391029641_10154903433019641   \n",
       "3036             138  10154902391029641_10154902472169641   \n",
       "3037             138  10154902391029641_10154902757359641   \n",
       "3038             138  10154902391029641_10154902521774641   \n",
       "3039             138  10154902391029641_10154903584594641   \n",
       "3040             138  10154902391029641_10154903474139641   \n",
       "3041             138  10154902391029641_10154902434639641   \n",
       "3042             138  10154902391029641_10154903987854641   \n",
       "3043             138  10154902391029641_10154904205654641   \n",
       "3044             138  10154902391029641_10154902825839641   \n",
       "3045             138  10154902391029641_10154902533434641   \n",
       "3046             138  10154902391029641_10154902985364641   \n",
       "3047             138  10154902391029641_10154905705679641   \n",
       "3048             138  10154902391029641_10154902432389641   \n",
       "3049             138  10154902391029641_10154903473514641   \n",
       "3050             138  10154902391029641_10154903717249641   \n",
       "\n",
       "                                        comment message  \\\n",
       "0     Sorry for yr loss I hope u all charish every m...   \n",
       "1     So sorry for your loss. As a mother of twins, ...   \n",
       "2                              This breaks my heart, 😧😦   \n",
       "3                                 Sorry for your loss!!   \n",
       "4                                  It breaks my heart 😢   \n",
       "5                                   My heart is broken!   \n",
       "6                                 It breaks my heart ❤️   \n",
       "7                                           cute baby..   \n",
       "8               Beautiful memories. Terribly saddening.   \n",
       "9                                    So sorry, Blessing   \n",
       "10                                                        \n",
       "11                                            Fake News   \n",
       "12                                                        \n",
       "13                               Beautiful in every way   \n",
       "14    Beautiful baby in heaven !!! my grandbaby is i...   \n",
       "15                                               Bellos   \n",
       "16                                              so cute   \n",
       "17                                            sleep now   \n",
       "18           i feel dearly for the parents of the child   \n",
       "19                                    such a sad moment   \n",
       "20                                         Imat Belarde   \n",
       "21    Distracted daily basics,,,,,https://m.facebook...   \n",
       "22                              Eswaran Balasubramanian   \n",
       "23    Good! Let's get those animals back in the wild...   \n",
       "24    Circuses, zoos, aquatic parks, and anything el...   \n",
       "25    Sadly! No support from audience who love to wa...   \n",
       "26    While it is a victory for animal activists. I ...   \n",
       "27    While the animals retiring is good for them, I...   \n",
       "28    Thank you God for finally making us realize th...   \n",
       "29    Circuses were ok in the early 1900's for famil...   \n",
       "...                                                 ...   \n",
       "3021                                     Michael Carney   \n",
       "3022                              Jean Paul Langenstein   \n",
       "3023                                         Nidal Eses   \n",
       "3024                                     Amy Therriault   \n",
       "3025                                               Nick   \n",
       "3026                                  Ann Marie Kennedy   \n",
       "3027                            Gregory James Gotthardt   \n",
       "3028                                         Mike Penna   \n",
       "3029                                  Karma Wangyal Grg   \n",
       "3030                                    Brandon Bonilla   \n",
       "3031                                    Adineri Marquez   \n",
       "3032                                        Aaron Zeier   \n",
       "3033                                   Sebastian Castro   \n",
       "3034                                       Matthew Bell   \n",
       "3035                                           Gary Lee   \n",
       "3036                                         Gui Lherme   \n",
       "3037                                    Astrid Francoys   \n",
       "3038                            Bailey Anabelle Lesslie   \n",
       "3039                                   Guillaume Landry   \n",
       "3040                                   Dominique Vallée   \n",
       "3041                               Brittany Brandi Best   \n",
       "3042                                       Sara Nilsson   \n",
       "3043                                        Kent Larsen   \n",
       "3044                                    Jamie Buttigieg   \n",
       "3045                                        Silas Ayers   \n",
       "3046  LA FLAMME DE WILLY MUFFLER EST ACTIVÉE POUR LE...   \n",
       "3047                                 Krystle Kaye Lemly   \n",
       "3048  FLAT EARTH  -- LOOK AT THE RAINBOW!\\nhttps://w...   \n",
       "3049                                      Jordan Moisan   \n",
       "3050                                       Tobi Naumann   \n",
       "\n",
       "                    comment time  comment likes  \n",
       "0     2017-01-15 13:44:40.000000              1  \n",
       "1     2017-01-15 13:38:48.000000              0  \n",
       "2     2017-01-15 13:18:41.000000              1  \n",
       "3     2017-01-15 13:34:48.000000              1  \n",
       "4     2017-01-15 13:06:35.000000              4  \n",
       "5     2017-01-15 13:31:46.000000              0  \n",
       "6     2017-01-15 13:24:53.000000              0  \n",
       "7     2017-01-15 14:00:51.000000              0  \n",
       "8     2017-01-15 13:56:43.000000              0  \n",
       "9     2017-01-15 13:46:00.000000              0  \n",
       "10    2017-01-15 13:37:09.000000              0  \n",
       "11    2017-01-15 13:41:11.000000              0  \n",
       "12    2017-01-15 13:28:21.000000              0  \n",
       "13    2017-01-15 13:08:43.000000              2  \n",
       "14    2017-01-15 13:27:22.000000              0  \n",
       "15    2017-01-15 13:43:36.000000              0  \n",
       "16    2017-01-15 13:03:39.000000              2  \n",
       "17    2017-01-15 13:11:22.000000              3  \n",
       "18    2017-01-15 14:03:23.000000              0  \n",
       "19    2017-01-15 14:02:43.000000              0  \n",
       "20    2017-01-15 13:46:14.000000              1  \n",
       "21    2017-01-15 13:45:43.000000              0  \n",
       "22    2017-01-15 13:32:32.000000              0  \n",
       "23    2017-01-15 10:20:39.000000            102  \n",
       "24    2017-01-15 11:02:45.000000             47  \n",
       "25    2017-01-15 12:41:30.000000              8  \n",
       "26    2017-01-15 12:13:14.000000              6  \n",
       "27    2017-01-15 10:37:30.000000             17  \n",
       "28    2017-01-15 12:45:25.000000              4  \n",
       "29    2017-01-15 12:13:16.000000              6  \n",
       "...                          ...            ...  \n",
       "3021  2017-01-15 03:20:27.000000              0  \n",
       "3022  2017-01-15 01:48:31.000000              0  \n",
       "3023  2017-01-14 08:33:17.000000              0  \n",
       "3024  2017-01-14 19:55:39.000000              0  \n",
       "3025  2017-01-14 14:34:25.000000              0  \n",
       "3026  2017-01-14 08:24:02.000000              0  \n",
       "3027  2017-01-14 15:26:18.000000              0  \n",
       "3028  2017-01-14 07:34:34.000000              0  \n",
       "3029  2017-01-15 09:58:11.000000              0  \n",
       "3030  2017-01-14 22:00:44.000000              0  \n",
       "3031  2017-01-14 07:43:59.000000              1  \n",
       "3032  2017-01-14 16:13:18.000000              0  \n",
       "3033  2017-01-15 13:45:11.000000              0  \n",
       "3034  2017-01-15 07:37:29.000000              1  \n",
       "3035  2017-01-14 15:30:34.000000              0  \n",
       "3036  2017-01-14 08:09:45.000000              1  \n",
       "3037  2017-01-14 10:30:30.000000              1  \n",
       "3038  2017-01-14 08:38:35.000000              1  \n",
       "3039  2017-01-14 16:32:16.000000              0  \n",
       "3040  2017-01-14 15:49:06.000000              0  \n",
       "3041  2017-01-14 07:50:05.000000              1  \n",
       "3042  2017-01-14 19:08:40.000000              0  \n",
       "3043  2017-01-14 20:34:04.000000              0  \n",
       "3044  2017-01-14 10:56:49.000000              1  \n",
       "3045  2017-01-14 08:43:56.000000              0  \n",
       "3046  2017-01-14 12:12:00.000000              0  \n",
       "3047  2017-01-15 09:00:15.000000              0  \n",
       "3048  2017-01-14 07:48:59.000000              0  \n",
       "3049  2017-01-14 15:48:50.000000              1  \n",
       "3050  2017-01-14 17:32:40.000000              0  \n",
       "\n",
       "[3051 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#page='cnninternational'\n",
    "#lastPostsReactions(page,'CNN',20)\n",
    "filename='facebookCNN.sqlite'\n",
    "fb=getTable('facebookCNN.sqlite')\n",
    "getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donald Trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-235f77d602d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DonaldTrump'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlastPostsReactions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DT'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'facebookDT.sqlite'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#getAllGraphsForPage(filename,50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-233-788e4fbb5e95>\u001b[0m in \u001b[0;36mlastPostsReactions\u001b[0;34m(page, filename, n)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mserie\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mfb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAllComments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_comments_per_post\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mserie\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-232-74ead1fdf014>\u001b[0m in \u001b[0;36mgetAllComments\u001b[0;34m(postId, nb_comments_per_post, serie, fb)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnb_comments_per_post\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mpost_comments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_comments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    473\u001b[0m         }\n\u001b[1;32m    474\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    421\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m                 )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[1;32m    593\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/valentine/Documents/EPFL/MA1/Data_Science/DS_env/lib/python3.5/site-packages/requests/packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m    922\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                   self.__class__)\n\u001b[0;32m--> 924\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.5.0/Frameworks/Python.framework/Versions/3.5/lib/python3.5/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \"\"\"\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "page='DonaldTrump'\n",
    "lastPostsReactions(page,'DT',20)\n",
    "filename='facebookDT.sqlite' \n",
    "#getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hillary Clinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = 'hillaryclinton'\n",
    "lastPostsReactions(page,'HC',20)\n",
    "filename='facebookHC.sqlite'\n",
    "def getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9GAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page='9gag'\n",
    "lastPostsReactions(page,'9GAG',1)\n",
    "filename='faceboo9GAG.sqlite'\n",
    "#getAllGraphsForPage(filename,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Love What Really Matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page='lovewhatreallymatters'\n",
    "lastPostsReactions(page,'LOVE',20)\n",
    "filename='facebooLOVE.sqlite'\n",
    "getAllGraphsForPage(filename,50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
